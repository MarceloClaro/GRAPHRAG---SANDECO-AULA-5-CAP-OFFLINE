# ğŸš€ Iniciar GraphRAG - Guia RÃ¡pido

## âœ… ConfiguraÃ§Ã£o AutomÃ¡tica COMPLETA

- âœ“ `.env.local` criado e configurado
- âœ“ Ollama Local selecionado (gratuito, offline)
- âœ“ PDF de exemplo: `public/exemplo-teste.pdf`
- âœ“ Servidor rodando em `http://localhost:3000`

## ğŸ“‹ Passos para Usar

### 1. Abra o App
```
http://localhost:3000
```

### 2. Configure Provider (Opcional - jÃ¡ Ã© Ollama)
- Clique em âš™ï¸ **ConfiguraÃ§Ãµes**
- Verifique que **Ollama (Local)** estÃ¡ selecionado âœ“

### 3. FaÃ§a Upload de PDF
- Clique em **"Selecionar Arquivos"**
- Escolha seu PDF ou use o exemplo: `public/exemplo-teste.pdf`

### 4. Processe o Pipeline

#### Etapa 1: Enriquecimento IA
```
Clique em: "Limpar & Classificar com Ollama"
Status: Processando texto...
```

#### Etapa 2: Embeddings
```
Clique em: "Gerar Embeddings"
Status: Vetorizando conteÃºdo...
```

#### Etapa 3: CNN Training (AutomÃ¡tico)
```
Status: Refinando vetores com Triplet Loss
Epochs: 1-15
```

#### Etapa 4: ClusterizaÃ§Ã£o
```
Clique em: "Executar ClusterizaÃ§Ã£o"
Status: K-Means++ com Silhueta
```

#### Etapa 5: Grafo de Conhecimento
```
Clique em: "Construir Grafo"
Status: Criando arestas ponderadas
```

#### Etapa 6: RelatÃ³rio
```
Clique em: "RelatÃ³rio TÃ©cnico"
Visualize anÃ¡lises completas
```

#### Etapa 7: ExportaÃ§Ã£o
```
CSV Unificado: "Exportar CSV Unificado"
RelatÃ³rio PDF: "RelatÃ³rio PDF"
Auditoria XLSX: "Auditoria XLSX"
```

## ğŸ¤– Verificar ConfiguraÃ§Ã£o

Execute o script de automaÃ§Ã£o:
```bash
npm run automate
```

SaÃ­da esperada:
```
âœ“ .env.local carregado
âœ“ Ollama Local: Configurado
âœ“ PDF de teste: DisponÃ­vel
âœ“ Servidor: http://localhost:3000
```

## ğŸ“Š Dados Gerados

Cada etapa gera arquivos CSV:
- `etapa1_entidades_inteligentes.csv` - Chunks enriquecidos
- `etapa2_embeddings.csv` - Vetores
- `etapa4_clusters.csv` - Grupos semÃ¢nticos
- `etapa6_grafo_nos.csv` - NÃ³s do grafo
- `etapa6_grafo_arestas.csv` - Conectividades
- `pipeline_unificado.csv` - Dados consolidados

## ğŸ”§ Trocar para Google Gemini

Se tiver API key do Gemini:

1. Edite `.env.local`:
```env
VITE_GEMINI_API_KEY=AIzaSy...seu_token...
```

2. No app, vÃ¡ em **âš™ï¸ ConfiguraÃ§Ãµes**

3. Clique em **"Google Gemini"** (botÃ£o cinza)

4. Salve e recarregue a pÃ¡gina

## ğŸ› Troubleshooting

**App estÃ¡ em branco?**
- Pressione F12 para abrir console
- Procure por erros vermelhos
- Recarregue a pÃ¡gina (F5)

**Ollama nÃ£o conecta?**
- Verifique se `ollama serve` estÃ¡ rodando
- Instale modelos: `ollama pull llama3.2:3b`
- Teste: `http://localhost:11434/api/models`

**Muito lento?**
- Use um PDF pequeno primeiro
- CPU Ã© limitado, seja paciente com Ollama local
- Considere usar Google Gemini para velocidade

## ğŸ“š Estrutura do Projeto

```
/src
â”œâ”€â”€ App.tsx              (Interface principal)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ geminiService.ts (Google IA)
â”‚   â”œâ”€â”€ ollamaService.ts (Local IA)
â”‚   â””â”€â”€ ...
â””â”€â”€ components/          (React UI)

/public
â””â”€â”€ exemplo-teste.pdf    (PDF para teste)

/scripts
â””â”€â”€ automate.js          (VerificaÃ§Ã£o do sistema)

.env.local              (ConfiguraÃ§Ã£o)
```

## ğŸ¯ PrÃ³ximos Passos

1. Acesse `http://localhost:3000`
2. Selecione um PDF
3. Escolha um provider (Ollama Ã© padrÃ£o)
4. Clique nos botÃµes em sequÃªncia
5. Exporte os resultados

**Framework pronto para usar!** ğŸš€
