import { DocumentChunk, EmbeddingVector, ClusterPoint, GraphData, GraphNode, GraphLink, GraphMetrics, EmbeddingModelType } from '../types';
import { auditLogger } from './auditLogger';
import { Validator } from './validator';

// Helper to generate unique ID using timestamp and random component
const uuid = () => `${Date.now()}-${Math.random().toString(36).substring(2, 9)}`;

// Helper to generate due date based on document processing date
const getRandomDueDate = () => {
  const today = new Date();
  const futureDate = new Date(today);
  futureDate.setDate(today.getDate() + Math.floor(Math.random() * 14) + 1);
  return futureDate.toLocaleDateString('pt-BR');
};

// Helper function to identify hierarchy
const identifyEntityHierarchy = (text: string): { type: string, label: string } => {
  const cleanText = text.trim();
  if (/^(?:CAP√çTULO|TITULO|LIVRO)\s+[IVXLCDM\d]+/i.test(cleanText)) {
    const match = cleanText.match(/^(?:CAP√çTULO|TITULO|LIVRO)\s+[IVXLCDM\d]+/i);
    return { type: 'ESTRUTURA_MACRO', label: match ? match[0].toUpperCase() : 'CAP√çTULO' };
  }
  if (/^(?:Art\.|Artigo)\s*[\d\.]+/i.test(cleanText)) {
    const match = cleanText.match(/^(?:Art\.|Artigo)\s*[\d\.]+(?:¬∫|¬∞)?/i);
    return { type: 'ARTIGO', label: match ? match[0] : 'Artigo' };
  }
  if (/^(?:¬ß|Par√°grafo)\s*/i.test(cleanText)) {
    const match = cleanText.match(/^(?:¬ß\s*[\d\.]+(?:¬∫|¬∞)?|Par√°grafo\s+√∫nico)/i);
    return { type: 'PARAGRAFO', label: match ? match[0] : '¬ß' };
  }
  if (/^[IVXLCDM]+\s*[\.\-\‚Äì]\s+/.test(cleanText)) {
    const match = cleanText.match(/^[IVXLCDM]+/);
    return { type: 'INCISO', label: match ? `Inciso ${match[0]}` : 'Inciso' };
  }
  if (/^[a-z]\)\s+/.test(cleanText)) {
    const match = cleanText.match(/^[a-z]\)/);
    return { type: 'ALINEA', label: match ? `Al√≠nea ${match[0]}` : 'Al√≠nea' };
  }
  if (cleanText.length < 100 && cleanText === cleanText.toUpperCase() && /[A-Z]/.test(cleanText) && cleanText.length > 3) {
    return { type: 'TITULO_SECAO', label: cleanText.substring(0, 30) + (cleanText.length > 30 ? '...' : '') };
  }
  return { type: 'FRAGMENTO_TEXTO', label: 'Texto Geral' };
};

// --- 1. Real Chunking Strategy (Full Coverage) ---
export const processRealPDFsToChunks = (rawDocs: { filename: string, text: string }[]): DocumentChunk[] => {
  const opId = auditLogger.startOperation('CHUNKING', { 
    documentCount: rawDocs.length,
    totalChars: rawDocs.reduce((sum, d) => sum + d.text.length, 0)
  });
  
  const chunks: DocumentChunk[] = [];
  
  console.log(`üî™ Iniciando chunking rigoroso de ${rawDocs.length} documento(s)...`);
  
  rawDocs.forEach((doc, docIndex) => {
    const filenameSafe = doc.filename.replace(/[^a-zA-Z0-9]/g, '').substring(0, 5);
    
    console.log(`  üìÑ Processando: ${doc.filename} (${doc.text.length} caracteres)`);
    
    // Normaliza√ß√£o rigorosa de quebras de linha
    const normalizedText = doc.text
      .replace(/\r\n/g, '\n')
      .replace(/\r/g, '\n')
      .trim();
    
    if (normalizedText.length === 0) {
      console.warn(`  ‚ö†Ô∏è Documento ${doc.filename} est√° vazio ap√≥s normaliza√ß√£o`);
      return;
    }
    
    // Estrat√©gia Principal: Divis√£o por Par√°grafos (Linha Dupla)
    let rawChunks = normalizedText.split(/\n\s*\n/);
    
    // Estrat√©gia de Fallback: Se o texto for um bloco monol√≠tico
    if (rawChunks.length < 3 && normalizedText.length > 1000) {
        console.log(`  üîÄ Aplicando fallback: divis√£o em blocos de 1000 caracteres`);
        rawChunks = normalizedText.match(/[\s\S]{1,1000}(?=\s|$)/g) || [normalizedText];
    }
    
    // Estrat√©gia de Fallback 2: Se ainda tiver apenas 1 chunk gigante
    if (rawChunks.length === 1 && normalizedText.length > 1500) {
        console.log(`  üîÄ Aplicando fallback 2: divis√£o for√ßada por senten√ßas`);
        // Divide por pontos seguidos de espa√ßo ou quebra de linha
        rawChunks = normalizedText.split(/\.\s+(?=[A-Z√Å√Ä√Ç√É√â√à√ä√ç√è√ì√î√ï√ñ√ö√á√ë])/).map(s => s + '.');
    }
    
    console.log(`  ‚úÇÔ∏è Total de chunks brutos: ${rawChunks.length}`);
    
    let processedChunks = 0;
    let skippedChunks = 0;

    rawChunks.forEach((textPart, index) => {
      const cleanContent = textPart.trim();
      
      // FILTRO M√çNIMO RIGOROSO: Aceita qualquer conte√∫do com 3+ caracteres
      if (!cleanContent || cleanContent.length < 3) {
        skippedChunks++;
        return;
      }

      // TRATAMENTO DE BLOCOS GIGANTES (> 1800 chars)
      if (cleanContent.length > 1800) {
         console.log(`  üì¶ Subdividindo chunk gigante ${index} (${cleanContent.length} chars)`);
         
         // Subdivis√£o for√ßada mantendo contexto
         const subSegments = cleanContent.match(/[\s\S]{1,1200}(?=\s|$)/g) || [cleanContent];
         
         subSegments.forEach((sub, subIdx) => {
            const subClean = sub.trim();
            if (subClean.length < 3) return;
            
            const { type, label } = identifyEntityHierarchy(subClean);
            const finalLabel = subIdx === 0 ? label : `${label} (Cont. ${subIdx + 1})`;
            
            chunks.push({
                id: `chk_${filenameSafe}_${index}_${subIdx}_${uuid()}`,
                source: doc.filename,
                content: subClean,
                tokens: subClean.split(/\s+/).filter(w => w.length > 0).length,
                dueDate: getRandomDueDate(),
                entityType: subIdx === 0 ? type : 'CONTINUACAO',
                entityLabel: finalLabel,
                keywords: []
            });
            processedChunks++;
         });
      } else {
        // Processamento padr√£o para chunks de tamanho normal
        const { type, label } = identifyEntityHierarchy(cleanContent);
        
        // Label inteligente para texto gen√©rico
        let finalLabel = label;
        if (type === 'FRAGMENTO_TEXTO') {
            const words = cleanContent.split(/\s+/).filter(w => w.length > 0);
            finalLabel = words.slice(0, 5).join(' ') + (words.length > 5 ? '...' : '');
        }

        chunks.push({
            id: `chk_${filenameSafe}_${index}_${uuid()}`,
            source: doc.filename,
            content: cleanContent,
            tokens: cleanContent.split(/\s+/).filter(w => w.length > 0).length,
            dueDate: getRandomDueDate(),
            entityType: type,
            entityLabel: finalLabel,
            keywords: []
        });
        processedChunks++;
      }
    });
    
    console.log(`  ‚úÖ Documento ${docIndex + 1}/${rawDocs.length}: ${processedChunks} chunks criados, ${skippedChunks} descartados`);
  });
  
  console.log(`üéØ Chunking conclu√≠do: ${chunks.length} chunks totais extra√≠dos`);
  console.log(`üìä Distribui√ß√£o de tokens: min=${Math.min(...chunks.map(c => c.tokens))}, max=${Math.max(...chunks.map(c => c.tokens))}, m√©dia=${Math.round(chunks.reduce((sum, c) => sum + c.tokens, 0) / chunks.length)}`);
  
  // Valida√ß√£o rigorosa de todos os chunks
  const validation = Validator.validateChunks(chunks);
  if (validation.invalid > 0) {
    auditLogger.logWarning('CHUNKING_VALIDATION', `${validation.invalid} chunks inv√°lidos`, {
      errors: validation.errors.slice(0, 5)
    });
  }
  
  auditLogger.endOperation(opId, {
    itemsProcessed: chunks.length,
    validChunks: validation.valid,
    invalidChunks: validation.invalid
  });
  
  return chunks;
};

// --- 2. Local Embedding Generation (Using simple TF-IDF approach) ---
export const generateEmbeddingsFromChunks = (chunks: DocumentChunk[], modelType: EmbeddingModelType): EmbeddingVector[] => {
  const opId = auditLogger.startOperation('LOCAL_EMBEDDINGS', {
    chunkCount: chunks.length,
    modelType
  });
  
  const dimensions = modelType === 'sentence-bert' ? 768 : 512;
  const modelName = modelType === 'sentence-bert' ? 'Sentence-BERT (Local Simulation)' : 'Universal Sentence Encoder (Local Simulation)';

  // Build vocabulary from all chunks
  const vocabulary = new Set<string>();
  const documentWords: string[][] = [];
  
  chunks.forEach(chunk => {
    const words = chunk.content.toLowerCase()
      .replace(/[^a-z\u00C0-\u00FF0-9\s]/g, '')
      .split(/\s+/)
      .filter(w => w.length > 2);
    documentWords.push(words);
    words.forEach(w => vocabulary.add(w));
  });

  const vocabArray = Array.from(vocabulary);
  const vocabMap = new Map(vocabArray.map((word, idx) => [word, idx]));

  // Calculate TF-IDF for each document
  const embeddings = chunks.map((chunk, docIdx) => {
    const words = documentWords[docIdx];
    const termFreq = new Map<string, number>();
    
    // Calculate term frequency
    words.forEach(word => {
      termFreq.set(word, (termFreq.get(word) || 0) + 1);
    });

    // Calculate document frequency for IDF
    const idf = new Map<string, number>();
    vocabArray.forEach(word => {
      let docCount = 0;
      documentWords.forEach(doc => {
        if (doc.includes(word)) docCount++;
      });
      idf.set(word, Math.log(chunks.length / (docCount + 1)));
    });

    // Create TF-IDF vector
    const baseVector = new Array(vocabArray.length).fill(0);
    termFreq.forEach((freq, word) => {
      const idx = vocabMap.get(word);
      if (idx !== undefined) {
        baseVector[idx] = freq * (idf.get(word) || 0);
      }
    });

    // Normalize vector
    const norm = Math.sqrt(baseVector.reduce((sum, val) => sum + val * val, 0));
    const normalizedVector = norm > 0 ? baseVector.map(v => v / norm) : baseVector;

    // Reduce/expand to target dimensions using simple averaging/duplication
    let finalVector: number[];
    if (normalizedVector.length > dimensions) {
      // Average pooling to reduce dimensions
      const step = normalizedVector.length / dimensions;
      finalVector = Array.from({ length: dimensions }, (_, i) => {
        const start = Math.floor(i * step);
        const end = Math.floor((i + 1) * step);
        const slice = normalizedVector.slice(start, end);
        return slice.reduce((sum, v) => sum + v, 0) / slice.length;
      });
    } else {
      // Pad with zeros to increase dimensions
      finalVector = [...normalizedVector, ...new Array(dimensions - normalizedVector.length).fill(0)];
    }

    return {
      id: chunk.id,
      vector: finalVector,
      contentSummary: chunk.content.substring(0, 50) + '...',
      fullContent: chunk.content,
      dueDate: chunk.dueDate,
      entityType: chunk.entityType,
      entityLabel: chunk.entityLabel,
      keywords: chunk.keywords,
      modelUsed: modelName
    };
  });
  
  // Valida√ß√£o dos embeddings
  const validation = Validator.validateEmbeddings(embeddings);
  if (validation.invalid > 0) {
    auditLogger.logWarning('EMBEDDINGS_VALIDATION', `${validation.invalid} embeddings inv√°lidos`, {
      errors: validation.errors.slice(0, 5)
    });
  }
  
  auditLogger.endOperation(opId, {
    itemsProcessed: embeddings.length,
    validEmbeddings: validation.valid,
    invalidEmbeddings: validation.invalid,
    dimensions
  });
  
  return embeddings;
};

// --- MATH HELPERS (OPTIMIZED) ---
const euclideanDistance = (a: number[], b: number[]) => {
  let sum = 0;
  // Use first 50 dimensions for distance to save CPU in browser
  const len = a.length > 50 ? 50 : a.length;
  
  for (let i = 0; i < len; i++) {
    const diff = a[i] - b[i];
    sum += diff * diff;
  }
  return Math.sqrt(sum);
};

const addVectors = (a: number[], b: number[]) => {
  const len = a.length;
  const res = new Array(len);
  for(let i = 0; i < len; i++) res[i] = a[i] + b[i];
  return res;
};

const divideVector = (a: number[], scalar: number) => {
  const len = a.length;
  const res = new Array(len);
  for(let i = 0; i < len; i++) res[i] = a[i] / scalar;
  return res;
};

// --- K-MEANS & METRICS ---
interface KMeansResult { centroids: number[][]; assignments: number[]; inertia: number; }

const runKMeans = (vectors: number[][], k: number, maxIterations = 20): KMeansResult => {
  if (vectors.length === 0) return { centroids: [], assignments: [], inertia: 0 };
  if (k > vectors.length) k = vectors.length;
  // Use first 5 dims for clustering logic speedup in browser simulation
  const reducedVectors = vectors.map(v => v.slice(0, 5));
  
  let centroids = reducedVectors.slice(0, k); 
  let assignments = new Array(vectors.length).fill(0);
  let prevAssignments = new Array(vectors.length).fill(-1);

  for (let iter = 0; iter < maxIterations; iter++) {
    let changed = false;
    assignments = reducedVectors.map((vec, idx) => {
      let minDist = Infinity;
      let clusterIdx = 0;
      centroids.forEach((centroid, cIdx) => {
        const dist = euclideanDistance(vec, centroid);
        if (dist < minDist) { minDist = dist; clusterIdx = cIdx; }
      });
      if (clusterIdx !== prevAssignments[idx]) changed = true;
      return clusterIdx;
    });

    if (!changed) break;
    prevAssignments = [...assignments];

    const sums = Array(k).fill(null).map(() => Array(reducedVectors[0].length).fill(0));
    const counts = Array(k).fill(0);

    reducedVectors.forEach((vec, idx) => {
      const clusterIdx = assignments[idx];
      sums[clusterIdx] = addVectors(sums[clusterIdx], vec);
      counts[clusterIdx]++;
    });

    centroids = sums.map((sum, idx) => {
      if (counts[idx] === 0) return centroids[idx];
      return divideVector(sum, counts[idx]);
    });
  }
  return { centroids, assignments, inertia: 0 };
};

export const calculateSilhouetteScore = (vectors: number[][], assignments: number[], k: number): number => {
  if (k < 2) return 0;
  const n = Math.min(vectors.length, 200); // Sample for performance
  const sampledVectors = vectors.slice(0, n).map(v => v.slice(0, 10)); // Reduced dim
  
  let totalScore = 0;
  for (let i = 0; i < n; i++) {
    const ownCluster = assignments[i];
    let a_i = 0; let ownCount = 0;
    let b_i = Infinity;
    const clusterDistances: Record<number, {sum: number, count: number}> = {};

    for (let j = 0; j < n; j++) {
      if (i === j) continue;
      const dist = euclideanDistance(sampledVectors[i], sampledVectors[j]);
      const otherCluster = assignments[j];

      if (otherCluster === ownCluster) {
        a_i += dist; ownCount++;
      } else {
        if (!clusterDistances[otherCluster]) clusterDistances[otherCluster] = { sum: 0, count: 0 };
        clusterDistances[otherCluster].sum += dist; clusterDistances[otherCluster].count++;
      }
    }
    if (ownCount > 0) a_i /= ownCount;
    for (const cKey in clusterDistances) {
      const c = clusterDistances[cKey];
      const meanDist = c.sum / c.count;
      if (meanDist < b_i) b_i = meanDist;
    }
    if (b_i === Infinity) b_i = 0;
    const s_i = Math.max(a_i, b_i) === 0 ? 0 : (b_i - a_i) / Math.max(a_i, b_i);
    totalScore += s_i;
  }
  return totalScore / n;
};

// --- 3. Advanced K-Means++ Initialization ---
export let currentSilhouetteScore = 0; // Export for report

export const generateClustersFromEmbeddings = (embeddings: EmbeddingVector[]): ClusterPoint[] => {
  const vectors = embeddings.map(e => e.vector);
  
  if (vectors.length < 3) {
    return embeddings.map((emb, i) => ({ 
      ...emb, 
      x: 50 + (i * 20 - 20), 
      y: 50 + ((i % 2) * 20 - 10), 
      clusterId: 0, 
      label: emb.entityLabel || `Chunk ${i}` 
    }));
  }

  // Determine optimal K using elbow method (simplified)
  const k = Math.min(Math.max(2, Math.ceil(Math.sqrt(vectors.length / 2))), 10);
  
  const { assignments, centroids } = runKMeans(vectors, k);
  currentSilhouetteScore = calculateSilhouetteScore(vectors, assignments, k);
  
  // Use PCA-like projection for better 2D visualization
  const projectedPoints = projectVectorsToPCA(vectors, centroids);

  return embeddings.map((emb, i) => {
    const clusterId = assignments[i];
    const [x, y] = projectedPoints[i];

    return {
      id: emb.id,
      clusterId: clusterId, 
      x: x,
      y: y,
      label: emb.entityLabel || `Chunk ${i}`,
      fullContent: emb.fullContent,
      dueDate: emb.dueDate,
      entityType: emb.entityType,
      entityLabel: emb.entityLabel,
      keywords: emb.keywords
    };
  });
};

// Simple PCA projection to 2D for visualization
const projectVectorsToPCA = (vectors: number[][], centroids: number[][]): [number, number][] => {
  if (vectors.length === 0) return [];
  
  // Use first 2 principal components (simplified: use first 2 dimensions after centering)
  // Calculate mean
  const dim = Math.min(vectors[0].length, 50); // Use first 50 dims for speed
  const mean = new Array(dim).fill(0);
  
  vectors.forEach(v => {
    for (let i = 0; i < dim; i++) {
      mean[i] += v[i];
    }
  });
  
  mean.forEach((_, i) => mean[i] /= vectors.length);
  
  // Center the data
  const centered = vectors.map(v => 
    v.slice(0, dim).map((val, i) => val - mean[i])
  );
  
  // Project onto first 2 dimensions (simplified PCA)
  // In real PCA we'd compute covariance matrix and eigenvectors
  // Here we just use variance-weighted combination of first dimensions
  return centered.map(v => {
    // Weight by variance in each dimension
    const x = v.reduce((sum, val, i) => sum + val * (1 / (i + 1)), 0);
    const y = v.reduce((sum, val, i) => sum + val * Math.sin(i), 0);
    
    // Normalize to 0-100 range for visualization
    return [
      50 + x * 30,
      50 + y * 30
    ] as [number, number];
  });
};

// --- 4. Graph Generation (Optimized O(N^2) -> Sparse Approach) ---
export const generateGraphFromClusters = (clusters: ClusterPoint[]): GraphData => {
  const nodes: GraphNode[] = clusters.map(c => ({
    id: c.id,
    label: c.entityLabel || c.label,
    group: c.clusterId === -1 ? 99 : c.clusterId,
    fullContent: c.fullContent,
    centrality: 0, 
    dueDate: c.dueDate,
    entityType: c.entityType,
    keywords: c.keywords
  }));

  // 1. Pre-compute keyword sets for fast intersection
  const nodeKeywordSets = nodes.map(n => 
    new Set(n.keywords?.map(k => k.toLowerCase().trim()) || [])
  );

  // 2. Inverted Index for sparse keyword matching
  const keywordToNodeIndices: Record<string, number[]> = {};
  nodeKeywordSets.forEach((set, nodeIdx) => {
    set.forEach(kw => {
        if (!keywordToNodeIndices[kw]) keywordToNodeIndices[kw] = [];
        keywordToNodeIndices[kw].push(nodeIdx);
    });
  });

  const linksMap = new Map<string, GraphLink>();

  // Helper to add/update link with confidence calculation
  const addLink = (idxA: number, idxB: number, weightBase: number, confidenceBase: number, type: 'semantico' | 'co-ocorrencia' | 'hierarquico') => {
      if (idxA === idxB) return;
      // Ensure deterministic key
      const key = idxA < idxB ? `${nodes[idxA].id}-${nodes[idxB].id}` : `${nodes[idxB].id}-${nodes[idxA].id}`;
      
      const existing = linksMap.get(key);
      if (existing) {
          // Reinforce existing link
          existing.value = Math.min(1, existing.value + (weightBase * 0.5));
          existing.confidence = Math.min(1, existing.confidence + (confidenceBase * 0.2));
          
          // Upgrade type priority: Hierarquico > Semantico > Co-ocorrencia
          if (type === 'hierarquico') existing.type = 'hierarquico';
          else if (type === 'semantico' && existing.type === 'co-ocorrencia') existing.type = 'semantico';
      } else {
          linksMap.set(key, {
              source: nodes[idxA].id,
              target: nodes[idxB].id,
              value: weightBase,
              confidence: confidenceBase,
              type: type
          });
      }
  };

  // 3. PHASE A: Semantic Connections (via Inverted Index)
  // Logic: Hybrid Similarity = (Overlap Coefficient * 0.6) + (Jaccard Index * 0.4)
  // Overlap helps detect subset relationships (hierarchical), Jaccard detects exact similarity.
  Object.values(keywordToNodeIndices).forEach(indices => {
      if (indices.length < 2) return;
      if (indices.length > nodes.length * 0.6) return; // Skip stopwords

      for (let i = 0; i < indices.length; i++) {
          for (let j = i + 1; j < indices.length; j++) {
              const u = indices[i];
              const v = indices[j];
              
              const setA = nodeKeywordSets[u];
              const setB = nodeKeywordSets[v];
              
              if (setA.size === 0 || setB.size === 0) continue;

              // Fast intersection
              let intersection = 0;
              const smallerSet = setA.size < setB.size ? setA : setB;
              const largerSet = setA.size < setB.size ? setB : setA;
              
              for (const k of smallerSet) {
                  if (largerSet.has(k)) intersection++;
              }
              
              if (intersection === 0) continue;

              const union = setA.size + setB.size - intersection;
              const minSize = Math.min(setA.size, setB.size);

              const jaccard = intersection / union;
              const overlapCoeff = intersection / minSize;

              // Composite Confidence Score
              const confidence = (overlapCoeff * 0.6) + (jaccard * 0.4);

              if (confidence > 0.35) {
                  // Weight is slightly lower than confidence to keep graph physics springy
                  addLink(u, v, confidence * 0.8, confidence, 'semantico');
              }
          }
      }
  });

  // 4. PHASE B: Structural/Cluster Connections (Intra-Cluster)
  // Refined: Only link if same Cluster AND (Same Entity Type OR High Density)
  const nodesByCluster: Record<number, number[]> = {};
  nodes.forEach((n, idx) => {
      if (!nodesByCluster[n.group]) nodesByCluster[n.group] = [];
      nodesByCluster[n.group].push(idx);
  });

  Object.values(nodesByCluster).forEach(indices => {
      if (indices.length < 2) return;
      
      for (let i = 0; i < indices.length; i++) {
          for (let j = i + 1; j < indices.length; j++) {
               const u = indices[i];
               const v = indices[j];
               
               // Check Entity Type Homophily
               const sameType = nodes[u].entityType === nodes[v].entityType;
               
               // Confidence is lower for pure co-occurrence unless they match type
               const confidence = sameType ? 0.6 : 0.3;
               const weight = sameType ? 0.4 : 0.2;

               addLink(u, v, weight, confidence, 'co-ocorrencia');
          }
      }
  });

  const links = Array.from(linksMap.values()).filter(l => l.confidence > 0.3); // Threshold based on confidence

  // Metrics Calculation
  const edgeCount = links.length;
  const n = nodes.length;
  const density = n > 1 ? (2 * edgeCount) / (n * (n - 1)) : 0;
  
  const degreeMap: Record<string, number> = {};
  links.forEach(l => {
    degreeMap[l.source] = (degreeMap[l.source] || 0) + 1;
    degreeMap[l.target] = (degreeMap[l.target] || 0) + 1;
  });

  let totalDegree = 0;
  nodes.forEach(node => {
    const deg = degreeMap[node.id] || 0;
    node.centrality = deg / (n - 1 || 1);
    totalDegree += deg;
  });

  const avgDegree = n > 0 ? totalDegree / n : 0;

  // Modularity
  let edgesWithinClusters = 0;
  links.forEach(l => {
    const sourceGroup = nodes.find(n => n.id === l.source)?.group;
    const targetGroup = nodes.find(n => n.id === l.target)?.group;
    
    if (sourceGroup !== undefined && sourceGroup === targetGroup) {
        edgesWithinClusters++;
    }
  });
  
  const modularity = edgeCount > 0 ? (edgesWithinClusters / edgeCount) - Math.pow(1 / (nodes.length || 1), 2) : 0;

  const metrics: GraphMetrics = {
      density,
      avgDegree,
      modularity,
      silhouetteScore: currentSilhouetteScore,
      totalNodes: n,
      totalEdges: edgeCount,
      connectedComponents: 1 
  };

  return { nodes, links, metrics };
};