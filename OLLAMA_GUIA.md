# ü¶ô OLLAMA - IA Local Gratuita para CPU

## üìå O Que √© Ollama?

**Ollama** √© uma plataforma que permite executar **modelos de IA localmente** no seu computador, **sem necessidade de internet ou API keys**. √â ideal para:

- ‚úÖ **Uso totalmente gratuito** (sem limites)
- ‚úÖ **Privacidade total** (seus dados n√£o saem do computador)
- ‚úÖ **Funciona em CPUs** (n√£o precisa de GPU potente)
- ‚úÖ **Offline** (sem depend√™ncia de internet)
- ‚úÖ **Modelos otimizados** (Llama, Phi, Mistral, Gemma)

---

## üöÄ Instala√ß√£o do Ollama

### Windows
1. Baixe o instalador: https://ollama.com/download/windows
2. Execute o instalador
3. Ollama ser√° instalado e iniciado automaticamente

### macOS
```bash
brew install ollama
```
Ou baixe em: https://ollama.com/download/mac

### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

---

## üì• Instalando Modelos Recomendados

Ap√≥s instalar o Ollama, abra o **terminal/PowerShell** e execute:

### Para An√°lise de Texto (Enriquecimento)
```bash
# Llama 3.2 3B - Recomendado para CPU (2GB)
ollama pull llama3.2:3b

# OU Phi-3 Mini - Microsoft, excelente para CPU (2.3GB)
ollama pull phi3:mini

# OU Llama 3.2 1B - Ultra leve (1GB)
ollama pull llama3.2:1b
```

### Para Embeddings (Vetoriza√ß√£o)
```bash
# Nomic Embed - RECOMENDADO (768 dimens√µes, 274MB)
ollama pull nomic-embed-text

# OU All-MiniLM - Mais r√°pido (384 dimens√µes, 45MB)
ollama pull all-minilm
```

---

## ‚öôÔ∏è Configurando no GraphRAG Pipeline

### 1. Inicie o Ollama

O Ollama deve estar rodando. Verifique:

**Windows/Linux:**
```bash
ollama serve
```

**macOS:**
Ollama inicia automaticamente. Verifique com:
```bash
ollama list
```

### 2. Configure na Interface

1. Abra o GraphRAG Pipeline: `npm run dev`
2. Clique em **‚öôÔ∏è Configura√ß√µes** (canto superior direito)
3. Selecione **ü¶ô Ollama (Local)**
4. Configure:
   - **Endpoint:** `http://localhost:11434` (padr√£o)
   - **Modelo de Chat:** Escolha o que voc√™ instalou (ex: `llama3.2:3b`)
   - **Modelo de Embeddings:** Escolha o que voc√™ instalou (ex: `nomic-embed-text`)
5. Clique em **"Testar Conex√£o"** para verificar
6. Clique em **"Salvar Configura√ß√µes"**

---

## üéØ Modelos Recomendados por Uso

### Para CPUs Comuns (4-8GB RAM)
```bash
ollama pull llama3.2:3b       # Chat (2GB)
ollama pull nomic-embed-text  # Embeddings (274MB)
```
**Uso:** Ideal para laptops e PCs comuns. Bom equil√≠brio entre velocidade e qualidade.

### Para CPUs Fracas ou RAM Limitada (<4GB)
```bash
ollama pull llama3.2:1b       # Chat (1GB)
ollama pull all-minilm        # Embeddings (45MB)
```
**Uso:** Ultra leve, roda em quase qualquer m√°quina. Qualidade um pouco menor.

### Para CPUs Potentes (16GB+ RAM)
```bash
ollama pull mistral:7b        # Chat (4.1GB)
ollama pull mxbai-embed-large # Embeddings (670MB)
```
**Uso:** Melhor qualidade, mas mais lento. Recomendado para processamento de muitos documentos.

---

## üìä Compara√ß√£o: Gemini vs Ollama

| Aspecto | Google Gemini | Ollama |
|---------|---------------|--------|
| **Custo** | Gratuito (com limites) | Totalmente gratuito |
| **Internet** | Obrigat√≥ria | N√£o necess√°ria |
| **Privacidade** | Dados enviados √† Google | 100% local |
| **Qualidade** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente | ‚≠ê‚≠ê‚≠ê‚≠ê Muito boa |
| **Velocidade** | R√°pida (depende da internet) | M√©dia (depende do CPU) |
| **Limites** | 15 req/min, 1.5k req/dia | Sem limites |
| **Setup** | API Key necess√°ria | Instalar software |

---

## üîß Comandos √öteis do Ollama

### Listar modelos instalados
```bash
ollama list
```

### Remover um modelo
```bash
ollama rm llama3.2:3b
```

### Testar um modelo no terminal
```bash
ollama run llama3.2:3b
```
Digite uma pergunta e pressione Enter. Digite `/bye` para sair.

### Ver modelos dispon√≠veis
Acesse: https://ollama.com/library

---

## üí° Dicas de Uso

### Para Melhor Desempenho:
1. ‚úÖ Use modelos de 1-3B de par√¢metros em CPUs comuns
2. ‚úÖ Feche outros programas pesados enquanto processa
3. ‚úÖ Processe documentos em batches menores
4. ‚úÖ Use `nomic-embed-text` para embeddings (melhor custo-benef√≠cio)

### Para M√°xima Qualidade:
1. ‚úÖ Use `phi3:mini` ou `llama3.2:3b` para an√°lise
2. ‚úÖ Configure mais epochs no treinamento CNN
3. ‚úÖ Use `mxbai-embed-large` para embeddings (se tiver RAM)

### Para M√°xima Velocidade:
1. ‚úÖ Use `llama3.2:1b` (modelo mais leve)
2. ‚úÖ Use `all-minilm` para embeddings
3. ‚úÖ Reduza o n√∫mero de chunks processados por vez

---

## üêõ Resolu√ß√£o de Problemas

### Erro: "N√£o foi poss√≠vel conectar ao Ollama"

**Solu√ß√£o:**
```bash
# Inicie o servidor Ollama
ollama serve
```

Ou verifique se est√° rodando:
```bash
# Windows
Get-Process ollama

# Linux/Mac
ps aux | grep ollama
```

### Erro: "Modelo n√£o encontrado"

**Solu√ß√£o:**
```bash
# Instale o modelo
ollama pull llama3.2:3b
```

### Processamento muito lento

**Solu√ß√µes:**
1. Use um modelo menor (`llama3.2:1b`)
2. Processe menos documentos por vez
3. Feche outros programas
4. Verifique uso de CPU (deve estar em 100% durante processamento)

### Ollama consome muita RAM

**Solu√ß√µes:**
1. Use modelos menores
2. Remova modelos n√£o usados: `ollama rm nome_do_modelo`
3. Reinicie o Ollama: 
   ```bash
   # Windows
   Stop-Process -Name ollama
   ollama serve
   ```

---

## üìà Benchmarks (Tempo M√©dio)

Processando 100 chunks de documentos em CPU Intel i5:

| Modelo | Tempo (An√°lise) | Tempo (Embeddings) | RAM Usada |
|--------|----------------|-------------------|-----------|
| llama3.2:1b | ~8 min | - | ~1.5 GB |
| llama3.2:3b | ~12 min | - | ~3 GB |
| phi3:mini | ~10 min | - | ~2.5 GB |
| all-minilm | - | ~2 min | ~200 MB |
| nomic-embed-text | - | ~4 min | ~500 MB |
| mxbai-embed-large | - | ~7 min | ~1 GB |

---

## üîÑ Atualizando Modelos

```bash
# Atualizar um modelo para a vers√£o mais recente
ollama pull llama3.2:3b

# Atualizar todos os modelos
ollama list | awk '{print $1}' | tail -n +2 | xargs -I {} ollama pull {}
```

---

## üåê Links √öteis

- **Site Oficial:** https://ollama.com/
- **Modelos Dispon√≠veis:** https://ollama.com/library
- **Documenta√ß√£o:** https://github.com/ollama/ollama/blob/main/docs/api.md
- **Discord Comunidade:** https://discord.gg/ollama

---

## ‚úÖ Checklist de Setup

- [ ] Ollama instalado
- [ ] Servidor Ollama rodando (`ollama serve`)
- [ ] Modelo de chat instalado (ex: `llama3.2:3b`)
- [ ] Modelo de embeddings instalado (ex: `nomic-embed-text`)
- [ ] Configurado no GraphRAG Pipeline
- [ ] Conex√£o testada com sucesso
- [ ] Primeiro documento processado

---

## üéâ Vantagens do Ollama no GraphRAG

1. **‚úÖ Privacidade Total:** Seus documentos n√£o saem do computador
2. **‚úÖ Sem Custos:** Use quanto quiser, sem limites
3. **‚úÖ Offline:** Funciona sem internet
4. **‚úÖ CPU-Friendly:** Modelos otimizados para CPUs comuns
5. **‚úÖ Open Source:** Modelos de c√≥digo aberto (Llama, Phi, etc)

---

**ü¶ô Aproveite IA local, gratuita e privada com Ollama! üöÄ**
