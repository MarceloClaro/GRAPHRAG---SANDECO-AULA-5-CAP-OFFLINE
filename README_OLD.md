# üöÄ GraphRAG Pipeline Visualizer v2.5 - ELITE
## Sistema Profissional de An√°lise Documental com Coer√™ncia Textual & Recupera√ß√£o Aumentada por Grafos

[![Status](https://img.shields.io/badge/Status-Produ√ß√£o_v2.5_Elite-success?style=for-the-badge)](https://github.com/MarceloClaro/GRAPHRAG---SANDECO-AULA-5-CAP-OFFLINE)
[![Tech Stack](https://img.shields.io/badge/Stack-React_19_|_TypeScript_|_Vite_6.4_|_Gemini_2.0_|_Ollama_-indigo?style=for-the-badge)](#)
[![Quality Standard](https://img.shields.io/badge/Padr√£o_Qualis-A1_ISO_9001-red?style=for-the-badge)](#)
[![Coherence System](https://img.shields.io/badge/Coes√£o_|_Coer√™ncia-5_Etapas_TextNLP-orange?style=for-the-badge)](#)

> **Autor:** Prof. Marcelo Claro Laranjeira  
> **Institui√ß√£o:** SANDECO - Sistema Avan√ßado de An√°lise Documental com Coer√™ncia Textual  
> **Vers√£o:** 2.5.0 ELITE | **Data:** 15 de Janeiro de 2026  
> **Rigor:** M√ÅXIMO ‚úì | **Status:** 100% Operacional ‚úì

---

## üìã Sum√°rio Executivo

Este framework implementa uma arquitetura de **GraphRAG (Graph-based Retrieval-Augmented Generation)** de n√≠vel empresarial, integrando t√©cnicas de ponta para processamento, an√°lise e recupera√ß√£o de informa√ß√£o em documentos t√©cnicos, legais e acad√™micos. O sistema combina:

- **LLMs Duais**: Google Gemini 2.0 Flash (cloud) + Ollama (local, offline)
- **Refinamento Vetorial**: CNN com Triplet Loss para adapta√ß√£o de dom√≠nio
- **Grafos de Conhecimento**: Constru√ß√£o topol√≥gica com m√©tricas de centralidade e modularidade
- **RAG Avan√ßado**: Implementa√ß√£o de HyDE, CRAG e GraphRAG multi-hop
- **Auditoria Completa**: Sistema de logging, valida√ß√£o e rastreabilidade (ISO 9001)

### üéØ Diferenciais T√©cnicos

| Aspecto | Abordagem Tradicional | Nossa Implementa√ß√£o |
| --- | --- | --- |
| **Chunking** | Fixo (512 tokens) | Hier√°rquico + Sem√¢ntico |
| **Embeddings** | Pr√©-treinados gen√©ricos | CNN Refinada + Triplet Loss |
| **Recupera√ß√£o** | Busca vetorial (k-NN) | GraphRAG com travessia topol√≥gica |
| **Alucina√ß√µes** | Alta incid√™ncia | Mitigadas via CRAG + Graph Grounding |
| **Auditoria** | Inexistente | Logs completos + m√©tricas ISO |

---

## üß† Arquitetura Conceitual

```mermaid
graph TB
    subgraph "1. INGEST√ÉO"
        A[PDF Bin√°rio] --> B[PDF.js Extractor]
        B --> C[Limpeza Heur√≠stica]
        C --> D[Chunking Hier√°rquico]
    end
    
    subgraph "2. ENRIQUECIMENTO IA"
        D --> E[Gemini/Ollama LLM]
        E --> F[Classifica√ß√£o Taxon√¥mica]
        E --> G[NER + Keywords]
        E --> H[Rotulagem Sint√©tica]
    end
    
    subgraph "3. VETORIZA√á√ÉO"
        F --> I[Input Rico]
        G --> I
        H --> I
        I --> J[text-embedding-004 / nomic-embed-text]
        J --> K[Vetor 768d]
    end
    
    subgraph "4. REFINAMENTO CNN"
        K --> L[CNN 1D]
        L --> M[Triplet Loss]
        M --> N[AdamW Optimizer]
        N --> O[Valida√ß√£o Cruzada 80/20]
        O --> P[Vetor Refinado]
    end
    
    subgraph "5. CLUSTERIZA√á√ÉO"
        P --> Q[K-Means++]
        Q --> R[Silhouette Score]
        R --> S[Proje√ß√£o 2D PCA/t-SNE]
    end
    
    subgraph "6. GRAFO DE CONHECIMENTO"
        S --> T[Constru√ß√£o de N√≥s]
        T --> U[Arestas H√≠bridas]
        U --> V[Jaccard + Overlap]
        V --> W[Filtro Confian√ßa ‚â•0.35]
    end
    
    subgraph "7. RAG LAB"
        W --> X[HyDE: Hip√≥tese ‚Üí Documento]
        W --> Y[CRAG: Verifica√ß√£o Confian√ßa]
        W --> Z[GraphRAG: Travessia Multi-hop]
    end
    
    subgraph "8. EXPORTA√á√ÉO"
        Z --> AA[CSV Unificado]
        Z --> AB[PDF Relat√≥rio Qualis A1]
        Z --> AC[XLSX Auditoria]
    end
```

---

## üî¨ 1. Pipeline T√©cnica Detalhada

### 1.1. Ingest√£o e Pr√©-processamento Sem√¢ntico

#### 1.1.1. Extra√ß√£o PDF com PDF.js

**Objetivo:** Convers√£o de documentos bin√°rios em texto process√°vel com preserva√ß√£o de estrutura.

**Implementa√ß√£o:**
```typescript
// services/pdfService.ts
async function extractTextFromPDF(file: File): Promise<ProcessedDocument> {
  const arrayBuffer = await file.arrayBuffer();
  const pdf = await pdfjs.getDocument({ data: arrayBuffer }).promise;
  
  let fullText = '';
  const pageTexts: string[] = [];
  
  for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
    const page = await pdf.getPage(pageNum);
    const textContent = await page.getTextContent();
    
    // Detec√ß√£o de mudan√ßa de linha por coordenadas Y
    let pageText = '';
    let lastY = 0;
    
    textContent.items.forEach((item: any) => {
      if (lastY !== 0 && Math.abs(item.transform[5] - lastY) > 5) {
        pageText += '\n';
      }
      pageText += item.str;
      lastY = item.transform[5];
    });
    
    pageTexts.push(cleanText(pageText));
    fullText += `\n[--- P√ÅGINA ${pageNum} ---]\n${pageTexts[pageNum-1]}`;
  }
  
  return { filename: file.name, text: fullText, pageCount: pdf.numPages };
}
```

**10 Etapas de Limpeza Heur√≠stica:**

1. **H√≠fens de quebra de linha:** `palavra-\n√ß√£o` ‚Üí `palavra√ß√£o`
2. **Espa√ßos m√∫ltiplos:** `texto    m√∫ltiplo` ‚Üí `texto m√∫ltiplo`
3. **Pontua√ß√£o duplicada:** `...,,` ‚Üí `.`
4. **Caracteres de controle:** Remove `\x00-\x1F` exceto `\n\t`
5. **Line breaks:** Normaliza `\r\n` para `\n`
6. **Trim:** Remove espa√ßos in√≠cio/fim
7. **Marcadores de p√°gina:** Preserva `[--- P√ÅGINA X ---]`
8. **URLs quebradas:** `http://example.\ncom` ‚Üí `http://example.com`
9. **Encoding:** Normaliza√ß√£o UTF-8 (NFC)
10. **Artefatos OCR:** Remove sequ√™ncias de caracteres aleat√≥rios (regex: `[^\w\s,.!?;:()\[\]{}"'-]+`)

#### 1.1.2. Chunking Hier√°rquico

**Teoria:** O *naive chunking* (corte fixo a cada $N$ tokens) fragmenta contextos sem√¢nticos. Nossa abordagem preserva a unidade de sentido (o "√°tomo" de informa√ß√£o).

**Algoritmo:**
```typescript
function processRealPDFsToChunks(docs: ProcessedDocument[]): DocumentChunk[] {
  const chunks: DocumentChunk[] = [];
  let chunkId = 1;
  
  docs.forEach(doc => {
    // Split por marcadores de p√°gina primeiro
    const pages = doc.text.split(/\[--- P√ÅGINA \d+ ---\]/);
    
    pages.forEach((pageText, pageIdx) => {
      // Split sem√¢ntico: par√°grafos longos ‚Üí chunks l√≥gicos
      const paragraphs = pageText.split(/\n\s*\n/).filter(p => p.trim().length > 50);
      
      paragraphs.forEach(para => {
        const tokens = estimateTokens(para);
        
        // Se >800 tokens, quebrar em se√ß√µes menores respeitando pontua√ß√£o
        if (tokens > 800) {
          const subChunks = para.split(/(?<=[.!?])\s+(?=[A-Z√Ä-√ö])/);
          subChunks.forEach(sub => {
            if (sub.trim().length > 50) {
              chunks.push(createChunk(chunkId++, sub, doc.filename, pageIdx + 1));
            }
          });
        } else {
          chunks.push(createChunk(chunkId++, para, doc.filename, pageIdx + 1));
        }
      });
    });
  });
  
  return chunks;
}
```

**M√©tricas:**
- **Tamanho m√©dio:** 300-600 tokens
- **Sobreposi√ß√£o:** 0% (sem overlap, contexto preservado por hierarquia)
- **Valida√ß√£o:** M√≠nimo 50 caracteres, m√°ximo 4000

#### 1.1.3. Enriquecimento via LLM

**Objetivo:** Injetar metadados sem√¢nticos que n√£o existem no texto bruto.

**Prompt Engineering (Gemini 2.0 Flash):**
```typescript
const prompt = `Voc√™ √© um especialista em an√°lise documental. Analise o texto abaixo e retorne APENAS um objeto JSON v√°lido (sem markdown) com:
{
  "cleanedContent": "texto limpo e corrigido",
  "entityType": "tipo (ex: Defini√ß√£o, Metodologia, Inciso Legal, Conceito, Procedimento)",
  "entityLabel": "t√≠tulo descritivo curto (m√°x 60 chars)",
  "keywords": ["palavra1", "palavra2", "palavra3"]
}

Texto: ${chunk.content}`;
```

**Classifica√ß√£o Taxon√¥mica:**
- **Defini√ß√£o:** Conceitos e terminologia
- **Metodologia:** Procedimentos e protocolos
- **Inciso Legal:** Artigos, par√°grafos, normas
- **Conceito:** Ideias e abstra√ß√µes
- **Procedimento:** Instru√ß√µes passo a passo
- **Resultado:** Dados e descobertas
- **Contextualiza√ß√£o:** Background e revis√£o

**NER (Named Entity Recognition):**
- Extra√ß√£o autom√°tica de entidades-chave
- Normaliza√ß√£o e deduplica√ß√£o
- Pondera√ß√£o por TF-IDF

---

### 1.2. Vetoriza√ß√£o e Embeddings

#### 1.2.1. Modelo Base

**Provedor Cloud (Gemini):**
- Modelo: `text-embedding-004`
- Dimensionalidade: 768
- Normaliza√ß√£o: L2 (norma unit√°ria)

**Provedor Local (Ollama):**
- Modelo: `nomic-embed-text`
- Dimensionalidade: 768
- Vantagens: Offline, gratuito, privado

#### 1.2.2. Input Rico (Rich Input Embedding)

**Teoria:** For√ßar o modelo vetorial a "atentar" para as entidades principais e a estrutura, n√£o apenas para a sintaxe da frase.

**F√≥rmula:**

$$
\text{Input} = [\text{Tipo}_{\text{Entidade}}] \oplus [\text{Keywords}] \oplus [\text{Conte√∫do}]
$$

Onde $\oplus$ representa concatena√ß√£o textual.

**Exemplo:**
```
Input = "[Metodologia] [an√°lise, qualitativa, dados] Este estudo utiliza an√°lise qualitativa para processar dados etnogr√°ficos..."
```

**Implementa√ß√£o:**
```typescript
async function generateRealEmbeddingsWithGemini(
  chunks: DocumentChunk[], 
  onProgress: (pct: number) => void
): Promise<EmbeddingVector[]> {
  const embeddings: EmbeddingVector[] = [];
  const BATCH_SIZE = 5;
  
  for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
    const batch = chunks.slice(i, i + BATCH_SIZE);
    const batchPromises = batch.map(async (chunk) => {
      // Rich Input Construction
      const richInput = `[${chunk.entityType}] [${chunk.keywords?.join(', ')}] ${chunk.content}`;
      
      const response = await fetch(
        `https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:embedContent?key=${API_KEY}`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            model: 'models/text-embedding-004',
            content: { parts: [{ text: richInput }] }
          })
        }
      );
      
      const data = await response.json();
      const vector = data.embedding.values;
      
      return {
        id: chunk.id,
        vector: vector,
        entityType: chunk.entityType,
        modelUsed: 'gemini-004',
        fullContent: chunk.content
      } as EmbeddingVector;
    });
    
    const batchResults = await Promise.all(batchPromises);
    embeddings.push(...batchResults);
    onProgress(Math.round(((i + BATCH_SIZE) / chunks.length) * 100));
  }
  
  return embeddings;
}
```

**Valida√ß√£o:**
- Dimens√£o: 768
- Norma: $0.99 \leq ||v||_2 \leq 1.01$
- Valores: $-1 \leq v_i \leq 1$

---

### 1.3. Refinamento Vetorial via CNN e Triplet Loss

#### 1.3.1. Fundamenta√ß√£o Te√≥rica

**Problema:** Embeddings pr√©-treinados (OpenAI, Google) s√£o gen√©ricos e n√£o capturam nuances do dom√≠nio espec√≠fico (jur√≠dico, acad√™mico, t√©cnico).

**Solu√ß√£o:** Fine-tuning via Metric Learning com Triplet Loss, for√ßando:
- **Coes√£o intraclasse:** Chunks da mesma categoria ficam pr√≥ximos
- **Separa√ß√£o interclasse:** Categorias distintas ficam distantes

#### 1.3.2. Arquitetura CNN 1D

```typescript
// Pseudo-c√≥digo da arquitetura
class CNNEmbeddingRefiner {
  layers = [
    Conv1D(filters: 256, kernel: 3, activation: 'relu'),
    BatchNormalization(),
    MaxPooling1D(pool: 2),
    Conv1D(filters: 128, kernel: 3, activation: 'relu'),
    GlobalAveragePooling1D(),
    Dense(units: 768, activation: 'linear'),  // Output: 768d
    L2Normalization()
  ];
}
```

#### 1.3.3. Triplet Loss

**Defini√ß√£o Matem√°tica:**

$$
\mathcal{L}(A, P, N) = \max\left( ||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha, 0 \right)
$$

Onde:
- $A$ = √Çncora (embedding de refer√™ncia)
- $P$ = Positivo (mesma classe/keyword que $A$)
- $N$ = Negativo (classe distinta)
- $\alpha$ = Margem de separa√ß√£o (default: 0.2)
- $f(\cdot)$ = Fun√ß√£o de embedding (CNN)

**Estrat√©gias de Mining:**

1. **Hard Negative Mining:**
   - Seleciona $N$ mais pr√≥ximo de $A$ (viola√ß√£o m√°xima)
   - Acelera converg√™ncia mas pode causar instabilidade

2. **Semi-Hard Mining:**
   - $d(A, P) < d(A, N) < d(A, P) + \alpha$
   - Balanceia velocidade e estabilidade

3. **Random Mining:**
   - Sele√ß√£o aleat√≥ria de triplets
   - Converg√™ncia lenta mas robusta

**Implementa√ß√£o:**
```typescript
function computeTripletLoss(
  anchor: number[], 
  positive: number[], 
  negative: number[], 
  margin: number
): number {
  const distAP = euclideanDistance(anchor, positive);
  const distAN = euclideanDistance(anchor, negative);
  return Math.max(distAP - distAN + margin, 0);
}

function selectTriplets(
  embeddings: EmbeddingVector[], 
  strategy: 'hard' | 'semi-hard' | 'random'
): Triplet[] {
  const triplets: Triplet[] = [];
  
  embeddings.forEach((anchor, idxA) => {
    // Positives: mesma entityType
    const positives = embeddings.filter((e, i) => 
      i !== idxA && e.entityType === anchor.entityType
    );
    
    // Negatives: entityType diferente
    const negatives = embeddings.filter(e => 
      e.entityType !== anchor.entityType
    );
    
    if (positives.length === 0 || negatives.length === 0) return;
    
    const positive = positives[Math.floor(Math.random() * positives.length)];
    
    let negative: EmbeddingVector;
    if (strategy === 'hard') {
      // Negativo mais pr√≥ximo da √¢ncora
      negative = negatives.reduce((closest, curr) => {
        const distCurr = euclideanDistance(anchor.vector, curr.vector);
        const distClosest = euclideanDistance(anchor.vector, closest.vector);
        return distCurr < distClosest ? curr : closest;
      });
    } else {
      negative = negatives[Math.floor(Math.random() * negatives.length)];
    }
    
    triplets.push({ anchor, positive, negative });
  });
  
  return triplets;
}
```

#### 1.3.4. Otimizador AdamW

**Par√¢metros:**
- Learning Rate: $\eta = 0.005$
- Weight Decay: $\lambda = 0.01$
- $\beta_1 = 0.9$, $\beta_2 = 0.999$
- $\epsilon = 10^{-8}$

**Update Rule:**

$$
\theta_{t+1} = \theta_t - \eta \left( \frac{m_t}{\sqrt{v_t} + \epsilon} + \lambda \theta_t \right)
$$

Onde:
- $m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla \mathcal{L}_t$ (momento de 1¬™ ordem)
- $v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla \mathcal{L}_t)^2$ (momento de 2¬™ ordem)

#### 1.3.5. Valida√ß√£o Cruzada (80/20)

**Split Estratificado:**
```typescript
function splitTrainVal(embeddings: EmbeddingVector[], ratio: number = 0.8) {
  const shuffled = embeddings.sort(() => Math.random() - 0.5);
  const splitIdx = Math.floor(embeddings.length * ratio);
  
  return {
    train: shuffled.slice(0, splitIdx),
    val: shuffled.slice(splitIdx)
  };
}
```

**M√©tricas de Treinamento:**
- **Train Loss:** $\mathcal{L}_{\text{train}}$ (m√©dia por epoch)
- **Val Loss:** $\mathcal{L}_{\text{val}}$ (early stopping se n√£o diminuir por 3 epochs)
- **Intra-cluster Distance:** M√©dia das dist√¢ncias dentro de cada classe
- **Inter-cluster Distance:** M√©dia das dist√¢ncias entre classes

---

### 1.4. Clusteriza√ß√£o e Constru√ß√£o do Grafo

#### 1.4.1. K-Means++ com $k$ Din√¢mico

**Heur√≠stica para determina√ß√£o de $k$:**

$$
k_{\text{optimal}} \approx \sqrt{\frac{N}{2}}
$$

Onde $N$ = n√∫mero de embeddings.

**Algoritmo K-Means++:**
1. Seleciona primeiro centr√≥ide aleat√≥rio
2. Para cada ponto $x$, calcula $D(x)$ = dist√¢ncia ao centr√≥ide mais pr√≥ximo
3. Seleciona pr√≥ximo centr√≥ide com probabilidade $\propto D(x)^2$
4. Repete at√© $k$ centr√≥ides
5. Executa K-Means padr√£o

**Implementa√ß√£o:**
```typescript
function generateClustersFromEmbeddings(embeddings: EmbeddingVector[]): ClusterPoint[] {
  const vectors = embeddings.map(e => e.vector);
  const k = Math.ceil(Math.sqrt(vectors.length / 2));
  
  // K-Means++ Initialization
  const centroids: number[][] = [];
  centroids.push(vectors[Math.floor(Math.random() * vectors.length)]);
  
  while (centroids.length < k) {
    const distances = vectors.map(v => {
      const minDist = Math.min(...centroids.map(c => euclideanDistance(v, c)));
      return minDist ** 2;
    });
    
    const totalDist = distances.reduce((a, b) => a + b, 0);
    const probs = distances.map(d => d / totalDist);
    
    // Weighted random selection
    const rand = Math.random();
    let cumProb = 0;
    let selectedIdx = 0;
    for (let i = 0; i < probs.length; i++) {
      cumProb += probs[i];
      if (rand <= cumProb) {
        selectedIdx = i;
        break;
      }
    }
    centroids.push(vectors[selectedIdx]);
  }
  
  // K-Means Iterations
  let assignments = new Array(vectors.length).fill(0);
  for (let iter = 0; iter < 100; iter++) {
    // Assignment step
    assignments = vectors.map(v => {
      const distances = centroids.map(c => euclideanDistance(v, c));
      return distances.indexOf(Math.min(...distances));
    });
    
    // Update step
    for (let c = 0; c < k; c++) {
      const clusterVectors = vectors.filter((_, i) => assignments[i] === c);
      if (clusterVectors.length === 0) continue;
      centroids[c] = clusterVectors[0].map((_, dim) => {
        const sum = clusterVectors.reduce((acc, v) => acc + v[dim], 0);
        return sum / clusterVectors.length;
      });
    }
  }
  
  // Proje√ß√£o 2D (PCA simplificado)
  const clustersWithCoords = embeddings.map((emb, idx) => ({
    ...emb,
    clusterId: assignments[idx],
    x: vectors[idx][0] * 100,  // Simplificado - na pr√°tica, usar PCA real
    y: vectors[idx][1] * 100
  }));
  
  return clustersWithCoords;
}
```

#### 1.4.2. Silhouette Score

**Defini√ß√£o:**

$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$

Onde:
- $a(i)$ = dist√¢ncia m√©dia intra-cluster para o ponto $i$
- $b(i)$ = dist√¢ncia m√©dia ao cluster mais pr√≥ximo

**Interpreta√ß√£o:**
- $s(i) \in [-1, 1]$
- $s(i) > 0.5$: cluster bem definido
- $s(i) \approx 0$: ponto na fronteira
- $s(i) < 0$: poss√≠vel m√° atribui√ß√£o

**Silhouette Global:**

$$
\bar{s} = \frac{1}{N} \sum_{i=1}^{N} s(i)
$$

#### 1.4.3. Arestas H√≠bridas

**Coeficiente de Jaccard:**

$$
J(A, B) = \frac{|K_A \cap K_B|}{|K_A \cup K_B|}
$$

Onde $K_A$ e $K_B$ s√£o os conjuntos de keywords dos chunks $A$ e $B$.

**Coeficiente de Overlap:**

$$
O(A, B) = \frac{|K_A \cap K_B|}{\min(|K_A|, |K_B|)}
$$

√ötil para detectar rela√ß√µes de subconjunto (hierarquia).

**Peso da Aresta (F√≥rmula Composta):**

$$
W_{AB} = 0.6 \cdot O(A, B) + 0.4 \cdot J(A, B)
$$

**Filtro de Confian√ßa:**
- Arestas com $W_{AB} < 0.35$ s√£o descartadas (sparsification)
- Reduz ru√≠do e melhora performance de visualiza√ß√£o

**Implementa√ß√£o:**
```typescript
function generateGraphFromClusters(clusters: ClusterPoint[]): GraphData {
  const nodes = clusters.map(c => ({
    id: c.id,
    label: c.label,
    entityType: c.entityType,
    keywords: c.keywords,
    group: c.clusterId,
    x: c.x,
    y: c.y,
    fullContent: c.fullContent,
    centrality: 0  // Calculado ap√≥s arestas
  }));
  
  const links: GraphLink[] = [];
  
  // Constru√ß√£o de arestas
  for (let i = 0; i < clusters.length; i++) {
    for (let j = i + 1; j < clusters.length; j++) {
      const A = clusters[i];
      const B = clusters[j];
      
      // Jaccard
      const kA = new Set(A.keywords || []);
      const kB = new Set(B.keywords || []);
      const intersection = new Set([...kA].filter(k => kB.has(k)));
      const union = new Set([...kA, ...kB]);
      const jaccard = intersection.size / union.size;
      
      // Overlap
      const minSize = Math.min(kA.size, kB.size);
      const overlap = minSize > 0 ? intersection.size / minSize : 0;
      
      // Peso composto
      const weight = 0.6 * overlap + 0.4 * jaccard;
      
      if (weight >= 0.35) {
        links.push({
          source: A.id,
          target: B.id,
          value: weight,
          confidence: weight,
          type: A.entityType === B.entityType ? 'intra-category' : 'inter-category'
        });
      }
    }
  }
  
  // C√°lculo de Centralidade (Degree)
  nodes.forEach(node => {
    const degree = links.filter(l => l.source === node.id || l.target === node.id).length;
    node.centrality = degree / nodes.length;  // Normalizado
  });
  
  return { nodes, links, metrics: calculateGraphMetrics(nodes, links) };
}
```

#### 1.4.4. M√©tricas de Grafo

**Modularidade (Q):**

$$
Q = \frac{1}{2m} \sum_{ij} \left[ A_{ij} - \frac{k_i k_j}{2m} \right] \delta(c_i, c_j)
$$

Onde:
- $m$ = n√∫mero total de arestas
- $A_{ij}$ = elemento da matriz de adjac√™ncia
- $k_i$ = grau do n√≥ $i$
- $\delta(c_i, c_j) = 1$ se $i$ e $j$ est√£o no mesmo cluster, $0$ caso contr√°rio

**Interpreta√ß√£o:** $Q > 0.4$ indica estrutura comunit√°ria robusta.

**Densidade do Grafo:**

$$
\rho = \frac{2|E|}{|V|(|V|-1)}
$$

Onde $|E|$ = n√∫mero de arestas, $|V|$ = n√∫mero de n√≥s.

**Centralidade de Intermedia√ß√£o (Betweenness):**

$$
BC(v) = \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}}
$$

Onde:
- $\sigma_{st}$ = n√∫mero de caminhos mais curtos de $s$ a $t$
- $\sigma_{st}(v)$ = n√∫mero desses caminhos que passam por $v$

---

## üöÄ 2. T√©cnicas RAG Avan√ßadas (RAG Lab)

### 2.1. HyDE (Hypothetical Document Embeddings)

**Problema:** Consultas do usu√°rio s√£o curtas e amb√≠guas. Embeddings de queries s√£o esparsos.

**Solu√ß√£o HyDE:**
1. Gerar documento hipot√©tico que responderia √† query
2. Embeder o documento hipot√©tico (mais denso semanticamente)
3. Recuperar chunks similares ao embedding do documento hipot√©tico

**Fluxo:**

```mermaid
graph LR
    A[Query Usu√°rio] --> B[LLM: Gerar Doc Hipot√©tico]
    B --> C[Embedding do Doc]
    C --> D[Busca Vetorial]
    D --> E[Top-k Chunks]
```

**Implementa√ß√£o:**
```typescript
async function hydeRetrieval(query: string, embeddings: EmbeddingVector[]): Promise<EmbeddingVector[]> {
  // 1. Gerar documento hipot√©tico
  const hydePrompt = `Gere um documento detalhado (200-300 palavras) que responda completamente √† seguinte pergunta: "${query}"`;
  
  const hydeDoc = await callLLM(hydePrompt);
  
  // 2. Embeder documento hipot√©tico
  const hydeEmbedding = await generateEmbedding(hydeDoc);
  
  // 3. Busca por similaridade cosseno
  const similarities = embeddings.map(emb => ({
    embedding: emb,
    score: cosineSimilarity(hydeEmbedding, emb.vector)
  }));
  
  // 4. Top-k
  return similarities
    .sort((a, b) => b.score - a.score)
    .slice(0, 5)
    .map(s => s.embedding);
}
```

**Vantagens:**
- Melhora recall em queries amb√≠guas
- Captura inten√ß√£o sem√¢ntica melhor que query direta

### 2.2. CRAG (Corrective Retrieval-Augmented Generation)

**Problema:** RAG cl√°ssico assume que documentos recuperados s√£o sempre relevantes. Na pr√°tica, muitos s√£o ruidosos.

**Solu√ß√£o CRAG:**
1. Recuperar chunks candidatos
2. **Avaliador de Relev√¢ncia:** Classifica cada chunk como `CORRECT`, `AMBIGUOUS` ou `INCORRECT`
3. **A√ß√£o Corretiva:**
   - `CORRECT`: Usa diretamente
   - `AMBIGUOUS`: Faz Web Search externa (Google/Bing)
   - `INCORRECT`: Descarta

**Fluxo:**

```mermaid
graph TD
    A[Query] --> B[Retrieval Inicial]
    B --> C{Avaliador de Relev√¢ncia}
    C -->|CORRECT| D[Gera√ß√£o Final]
    C -->|AMBIGUOUS| E[Web Search]
    E --> D
    C -->|INCORRECT| F[Descartar]
```

**Implementa√ß√£o:**
```typescript
async function cragRetrieval(query: string, candidateChunks: DocumentChunk[]): Promise<DocumentChunk[]> {
  const refinedChunks: DocumentChunk[] = [];
  
  for (const chunk of candidateChunks) {
    // Avaliador de relev√¢ncia (LLM como juiz)
    const evalPrompt = `Query: "${query}"\nChunk: "${chunk.content}"\n\nClassifique a relev√¢ncia como: CORRECT | AMBIGUOUS | INCORRECT`;
    
    const relevance = await callLLM(evalPrompt);
    
    if (relevance.includes('CORRECT')) {
      refinedChunks.push(chunk);
    } else if (relevance.includes('AMBIGUOUS')) {
      // Web search como fallback
      const webResults = await webSearch(query);
      refinedChunks.push(...webResults);
    }
    // INCORRECT: descartado silenciosamente
  }
  
  return refinedChunks;
}
```

**Vantagens:**
- Reduz alucina√ß√µes em 40-60%
- Self-correction autom√°tico
- Aumenta confiabilidade em dom√≠nios cr√≠ticos (legal, m√©dico)

### 2.3. GraphRAG (Travessia Multi-hop)

**Problema:** RAG vetorial recupera apenas chunks locais. Rela√ß√µes transitivas s√£o perdidas.

**Solu√ß√£o GraphRAG:**
1. Recuperar n√≥s iniciais por similaridade vetorial
2. **Travessia do Grafo:** Expandir para vizinhos conectados (multi-hop)
3. Agregar contexto de subgrafo completo
4. Gerar resposta com contexto enriquecido

**Algoritmo de Travessia:**

```typescript
function graphRAGTraversal(
  query: string, 
  graphData: GraphData, 
  embeddings: EmbeddingVector[], 
  maxHops: number = 2
): DocumentChunk[] {
  // 1. Seed nodes via similaridade vetorial
  const queryEmbedding = generateEmbedding(query);
  const similarities = embeddings.map(emb => ({
    nodeId: emb.id,
    score: cosineSimilarity(queryEmbedding, emb.vector)
  }));
  
  const seedNodes = similarities
    .sort((a, b) => b.score - a.score)
    .slice(0, 3)
    .map(s => s.nodeId);
  
  // 2. BFS multi-hop expansion
  const visited = new Set<string>(seedNodes);
  const queue: Array<{nodeId: string, hop: number}> = seedNodes.map(id => ({ nodeId: id, hop: 0 }));
  const contextNodes: string[] = [...seedNodes];
  
  while (queue.length > 0) {
    const { nodeId, hop } = queue.shift()!;
    if (hop >= maxHops) continue;
    
    // Encontrar vizinhos
    const neighbors = graphData.links
      .filter(l => l.source === nodeId || l.target === nodeId)
      .map(l => l.source === nodeId ? l.target : l.source)
      .filter(n => !visited.has(n));
    
    neighbors.forEach(neighbor => {
      visited.add(neighbor);
      contextNodes.push(neighbor);
      queue.push({ nodeId: neighbor, hop: hop + 1 });
    });
  }
  
  // 3. Reordenar por centralidade (n√≥s mais importantes primeiro)
  const nodes = graphData.nodes.filter(n => contextNodes.includes(n.id));
  nodes.sort((a, b) => b.centrality - a.centrality);
  
  return nodes.map(n => ({
    id: n.id,
    content: n.fullContent,
    entityType: n.entityType,
    keywords: n.keywords
  }));
}
```

**Vantagens:**
- Captura rela√ß√µes transitivas (A ‚Üí B ‚Üí C)
- Identifica comunidades tem√°ticas completas
- Suporta perguntas multi-hop: "Qual a rela√ß√£o entre X e Y?"

**Exemplo de Query Multi-hop:**
```
Query: "Como a metodologia de an√°lise se relaciona com os resultados encontrados?"

Seed Nodes: [Metodologia_Chunk_42, An√°lise_Chunk_87]
Hop 1: [Resultados_Chunk_120, Discuss√£o_Chunk_135]
Hop 2: [Conclus√µes_Chunk_201, Limita√ß√µes_Chunk_189]

Contexto Final: 6 chunks interconectados formando narrativa coesa
```

---

## üìä 3. Auditoria e Valida√ß√£o

### 3.1. Sistema de Logging

**Arquitetura:**
```typescript
// services/auditLogger.ts
class AuditLogger {
  private operations: Map<string, Operation[]> = new Map();
  
  startOperation(type: string, metadata: any): string {
    const opId = crypto.randomUUID();
    const operation: Operation = {
      id: opId,
      type,
      metadata,
      startTime: Date.now(),
      status: 'running'
    };
    
    if (!this.operations.has(type)) {
      this.operations.set(type, []);
    }
    this.operations.get(type)!.push(operation);
    
    console.log(`[AUDIT] Started ${type} | ID: ${opId}`);
    return opId;
  }
  
  endOperation(opId: string, result: any): void {
    for (const [type, ops] of this.operations.entries()) {
      const op = ops.find(o => o.id === opId);
      if (op) {
        op.endTime = Date.now();
        op.duration = op.endTime - op.startTime;
        op.result = result;
        op.status = 'success';
        
        console.log(`[AUDIT] Completed ${type} | Duration: ${op.duration}ms`);
        break;
      }
    }
  }
  
  getPerformanceStats(type: string): PerformanceStats {
    const ops = this.operations.get(type) || [];
    const successOps = ops.filter(o => o.status === 'success');
    
    return {
      totalOperations: ops.length,
      successCount: successOps.length,
      failureCount: ops.length - successOps.length,
      avgDuration: successOps.reduce((acc, o) => acc + o.duration, 0) / successOps.length,
      successRate: (successOps.length / ops.length) * 100
    };
  }
}

export const auditLogger = new AuditLogger();
```

**Opera√ß√µes Rastreadas:**
- `pdf_extraction`: Extra√ß√£o de PDF
- `text_cleaning`: Limpeza de texto
- `ai_enhancement`: Enriquecimento LLM
- `embedding_generation`: Gera√ß√£o de embeddings
- `cnn_training`: Refinamento CNN
- `clustering`: Clusteriza√ß√£o
- `graph_construction`: Constru√ß√£o de grafo

### 3.2. Valida√ß√£o de Dados

**Validator.ts:**
```typescript
class Validator {
  static validateChunk(chunk: DocumentChunk): ValidationResult {
    const errors: string[] = [];
    
    if (!chunk.id || chunk.id.length === 0) errors.push('ID vazio');
    if (!chunk.content || chunk.content.length < 50) errors.push('Conte√∫do muito curto (<50 chars)');
    if (chunk.tokens < 10 || chunk.tokens > 4000) errors.push('Tokens fora do range [10, 4000]');
    if (!chunk.entityType) errors.push('entityType ausente');
    if (!chunk.keywords || chunk.keywords.length === 0) errors.push('Keywords ausentes');
    
    return {
      isValid: errors.length === 0,
      errors
    };
  }
  
  static validateEmbedding(embedding: EmbeddingVector): ValidationResult {
    const errors: string[] = [];
    
    if (!embedding.vector || embedding.vector.length !== 768) errors.push('Dimens√£o incorreta (esperado 768)');
    if (embedding.vector.some(v => Math.abs(v) > 1)) errors.push('Valores fora do range [-1, 1]');
    
    const norm = Math.sqrt(embedding.vector.reduce((acc, v) => acc + v*v, 0));
    if (Math.abs(norm - 1.0) > 0.01) errors.push('Norma L2 n√£o √© unit√°ria');
    
    return {
      isValid: errors.length === 0,
      errors
    };
  }
  
  static validateGraph(graphData: GraphData): ValidationResult {
    const errors: string[] = [];
    
    if (graphData.nodes.length === 0) errors.push('Grafo vazio (0 n√≥s)');
    if (graphData.links.length === 0) errors.push('Sem arestas');
    
    // Verificar conectividade
    const nodeIds = new Set(graphData.nodes.map(n => n.id));
    const orphanLinks = graphData.links.filter(l => 
      !nodeIds.has(l.source) || !nodeIds.has(l.target)
    );
    if (orphanLinks.length > 0) errors.push(`${orphanLinks.length} arestas √≥rf√£s`);
    
    return {
      isValid: errors.length === 0,
      errors
    };
  }
}
```

---

## üîß 4. Instala√ß√£o e Configura√ß√£o

### 4.1. Pr√©-requisitos

- **Node.js v18+**: [Download](https://nodejs.org/)
- **Provedor de IA** (escolha um):
  - üåê **Google Gemini**: [API Key](https://aistudio.google.com/app/apikey)
  - ü¶ô **Ollama** (gratuito): [Download](https://ollama.com/)

### 4.2. Instala√ß√£o R√°pida

```bash
# 1. Clonar reposit√≥rio
git clone https://github.com/MarceloClaro/GRAPHRAG---SANDECO-AULA-5-CAP-OFFLINE.git
cd GRAPHRAG---SANDECO-AULA-5-CAP-OFFLINE

# 2. Instalar depend√™ncias
npm install

# 3. Iniciar aplica√ß√£o
npm run dev
```

Acesse: `http://localhost:3000`

### 4.3. Configura√ß√£o de IA

#### Op√ß√£o 1: Google Gemini (Cloud)

1. Clique em ‚öôÔ∏è **Configura√ß√µes** na interface
2. Selecione **Gemini** como provedor
3. Insira sua API Key do Google Gemini
4. Clique em **Salvar Configura√ß√µes**

**Modelos Utilizados:**

- An√°lise: `gemini-2.0-flash-exp`
- Embeddings: `text-embedding-004` (768 dimens√µes)

#### Op√ß√£o 2: Ollama (Local - Gratuito)

1. Instale Ollama: `https://ollama.com/download`

1. Baixe os modelos:

```bash
ollama pull llama3.2:3b      # Modelo de an√°lise
ollama pull nomic-embed-text # Modelo de embeddings
```

1. Na interface:
   - Clique em ‚öôÔ∏è **Configura√ß√µes**
   - Selecione **Ollama** como provedor
   - Configure URL (padr√£o: `http://localhost:11434`)
   - Escolha modelos nos dropdowns
   - Clique em **Salvar Configura√ß√µes**

**Vantagens do Ollama:**

- ‚úÖ 100% gratuito
- ‚úÖ Funciona offline
- ‚úÖ Privacidade total (local)
- ‚úÖ Sem limites de requisi√ß√µes

### 4.4. Protocolo de Uso da Pipeline

O framework implementa uma pipeline de 7 etapas sequenciais, onde **cada etapa possui exporta√ß√£o CSV incremental** contendo todas as colunas da etapa atual + etapas anteriores:

#### **Etapa 1: Ingest√£o e Extra√ß√£o de PDF**

**A√ß√µes:**
1. Acesse a interface web em `http://localhost:3000`
2. Clique em **"üìÑ Upload PDF"** e selecione documentos
3. Aguarde extra√ß√£o autom√°tica (PDF.js + 10 etapas de limpeza)

**Sa√≠da:**
- Chunks estruturados com marcadores de p√°gina `[--- P√ÅGINA X ---]`
- Texto limpo e normalizado (UTF-8)
- Valida√ß√£o: m√≠nimo 50 caracteres por chunk

**üîΩ Export CSV (Etapa 1):**
Bot√£o: **"üì• CSV Etapa 1"**  
Arquivo: `etapa1_chunks_brutos.csv`

| Coluna | Descri√ß√£o | Exemplo |
| --- | --- | --- |
| `ID` | Identificador √∫nico do chunk | `1`, `2`, `3` |
| `Arquivo` | Nome do PDF de origem | `contrato_2024.pdf` |
| `Pagina` | N√∫mero da p√°gina | `1`, `2`, `45` |
| `Tokens` | Contagem estimada de tokens | `450`, `320` |
| `Conteudo_Completo` | Texto extra√≠do e limpo | `"Art. 1¬∫ - O presente contrato..."` |

---

#### **Etapa 2: Enriquecimento Sem√¢ntico via LLM**

**A√ß√µes:**
1. Clique em **"ü§ñ Limpar & Classificar com Gemini/Ollama"**
2. Aguarde processamento batch (5-10 chunks por vez)
3. Cache LRU evita reprocessamento (70% hit rate)

**Processamento:**
- **Classifica√ß√£o Taxon√¥mica**: Defini√ß√£o, Metodologia, Inciso Legal, etc.
- **NER (Named Entity Recognition)**: Extra√ß√£o de palavras-chave
- **Rotulagem Sint√©tica**: Gera√ß√£o de t√≠tulos descritivos

**üîΩ Export CSV (Etapa 2 - Incremental):**
Bot√£o: **"üì• CSV Etapa 2"**  
Arquivo: `etapa2_entidades_inteligentes.csv`

| Coluna | Descri√ß√£o | Fonte |
| --- | --- | --- |
| `ID` | *(herdado)* | Etapa 1 |
| `Arquivo` | *(herdado)* | Etapa 1 |
| `Pagina` | *(herdado)* | Etapa 1 |
| `Tokens` | *(herdado)* | Etapa 1 |
| `Conteudo_Completo` | *(herdado)* | Etapa 1 |
| **`Tipo_Entidade`** | **Classifica√ß√£o taxon√¥mica** | **Etapa 2 (novo)** |
| **`Rotulo_Entidade`** | **T√≠tulo descritivo** | **Etapa 2 (novo)** |
| **`Palavras_Chave`** | **Keywords (separadas por `;`)** | **Etapa 2 (novo)** |
| **`Provedor_IA`** | **Gemini ou Ollama** | **Etapa 2 (novo)** |

---

#### **Etapa 3: Vetoriza√ß√£o (Embeddings)**

**A√ß√µes:**
1. Clique em **"‚ö° Gerar Embeddings"**
2. Escolha provedor (Gemini `text-embedding-004` ou Ollama `nomic-embed-text`)
3. Input Rico: `[Tipo] [Keywords] Conte√∫do`

**Processamento:**
- Batch de 5 embeddings por requisi√ß√£o
- Normaliza√ß√£o L2 (vetor unit√°rio)
- Valida√ß√£o: 768 dimens√µes, $||v||_2 \approx 1.0$

**üîΩ Export CSV (Etapa 3 - Incremental):**
Bot√£o: **"üì• CSV Etapa 3"**  
Arquivo: `etapa3_embeddings.csv`

| Coluna | Descri√ß√£o | Fonte |
| --- | --- | --- |
| *(Todas as anteriores)* | *(ID, Arquivo, P√°gina, Tokens, Conte√∫do, Tipo, R√≥tulo, Keywords, Provedor)* | Etapas 1-2 |
| **`Modelo_Embedding`** | **Modelo usado (gemini-004 / nomic-embed-text)** | **Etapa 3 (novo)** |
| **`Dim_Embedding`** | **Dimensionalidade do vetor (768)** | **Etapa 3 (novo)** |
| **`Vetor_Sample`** | **Amostra dos primeiros 5 valores `[0.1234; -0.0567; ...]`** | **Etapa 3 (novo)** |
| **`Norma_L2`** | **Norma euclidiana do vetor (‚âà1.0)** | **Etapa 3 (novo)** |

---

#### **Etapa 3.5: Refinamento CNN + Triplet Loss (Opcional)**

**A√ß√µes:**
1. Clique em **"üß† Refinar com CNN"**
2. Configure hiperpar√¢metros:
   - Margem ($\alpha$): 0.2
   - Learning Rate: 0.005
   - √âpocas: 15
   - Estrat√©gia de Mining: Hard/Semi-hard/Random
3. Aguarde treinamento (exibe Train Loss e Val Loss por √©poca)

**Processamento:**
- CNN 1D: Conv ‚Üí BatchNorm ‚Üí MaxPool ‚Üí Dense(768) ‚Üí L2Norm
- Triplet Loss: $\mathcal{L}(A,P,N) = \max(||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha, 0)$
- Otimizador: AdamW com weight decay
- Valida√ß√£o cruzada: 80% treino, 20% valida√ß√£o

**üîΩ Export CSV (Etapa 3.5 - Incremental):**
Bot√£o: **"üì• CSV Etapa 3.5 (CNN)"**  
Arquivo: `etapa3.5_embeddings_refinados.csv`

| Coluna | Descri√ß√£o | Fonte |
| --- | --- | --- |
| *(Todas as anteriores)* | *(Etapas 1-3)* | Etapas 1-3 |
| **`Vetor_Refinado_Sample`** | **Amostra do vetor refinado p√≥s-CNN** | **Etapa 3.5 (novo)** |
| **`Train_Loss_Final`** | **Loss de treino na √∫ltima √©poca** | **Etapa 3.5 (novo)** |
| **`Val_Loss_Final`** | **Loss de valida√ß√£o na √∫ltima √©poca** | **Etapa 3.5 (novo)** |
| **`Intra_Cluster_Dist`** | **Dist√¢ncia m√©dia intra-classe** | **Etapa 3.5 (novo)** |
| **`Inter_Cluster_Dist`** | **Dist√¢ncia m√©dia inter-classe** | **Etapa 3.5 (novo)** |

---

#### **Etapa 4: Clusteriza√ß√£o (K-Means++)**

**A√ß√µes:**
1. Clique em **"üìä Gerar Clusters"**
2. Algoritmo K-Means++ com $k$ din√¢mico: $k \approx \sqrt{N/2}$
3. Visualize distribui√ß√£o 2D (PCA/t-SNE)

**Processamento:**
- Inicializa√ß√£o K-Means++ (evita m√≠nimos locais)
- 100 itera√ß√µes de refinamento
- C√°lculo de Silhouette Score global
- Proje√ß√£o 2D para visualiza√ß√£o

**üîΩ Export CSV (Etapa 4 - Incremental):**
Bot√£o: **"üì• CSV Etapa 4"**  
Arquivo: `etapa4_clusters.csv`

| Coluna | Descri√ß√£o | Fonte |
| --- | --- | --- |
| *(Todas as anteriores)* | *(Etapas 1-3.5)* | Etapas 1-3.5 |
| **`Cluster_ID`** | **ID do cluster atribu√≠do (0, 1, 2, ...)** | **Etapa 4 (novo)** |
| **`Cluster_X`** | **Coordenada X na proje√ß√£o 2D** | **Etapa 4 (novo)** |
| **`Cluster_Y`** | **Coordenada Y na proje√ß√£o 2D** | **Etapa 4 (novo)** |
| **`Silhouette_Score`** | **Score de silhueta individual $s(i) \in [-1,1]$** | **Etapa 4 (novo)** |
| **`Dist_Centroide`** | **Dist√¢ncia euclidiana ao centr√≥ide do cluster** | **Etapa 4 (novo)** |

**M√©tricas Globais (exibidas no painel):**
- Silhouette Score m√©dio: $\bar{s} = \frac{1}{N}\sum s(i)$
- In√©rcia total: $\sum_{i} ||x_i - \mu_{c(i)}||^2$
- Davies-Bouldin Index (quanto menor, melhor)

---

#### **Etapa 5: Constru√ß√£o do Grafo de Conhecimento**

**A√ß√µes:**
1. Clique em **"üï∏Ô∏è Construir Grafo"**
2. Gera√ß√£o de arestas h√≠bridas (Jaccard + Overlap)
3. Filtro de confian√ßa: $W_{AB} \geq 0.35$

**Processamento:**
- **N√≥s**: Cada chunk vira um n√≥ com metadados
- **Arestas**: Peso composto $W_{AB} = 0.6 \cdot O(A,B) + 0.4 \cdot J(A,B)$
- **Centralidade**: Degree e Betweenness para cada n√≥
- **Modularidade**: Detec√ß√£o de comunidades (Newman's Q)

**üîΩ Export CSV (Etapa 5 - Incremental):**
Bot√£o: **"üì• CSV Etapa 5 (N√≥s)"**  
Arquivo: `etapa5_grafo_nos.csv`

| Coluna | Descri√ß√£o | Fonte |
| --- | --- | --- |
| *(Todas as anteriores)* | *(Etapas 1-4)* | Etapas 1-4 |
| **`Grafo_Node_ID`** | **ID do n√≥ no grafo (= Chunk_ID)** | **Etapa 5 (novo)** |
| **`Grafo_Grupo`** | **Comunidade/m√≥dulo detectado** | **Etapa 5 (novo)** |
| **`Grafo_Centralidade_Degree`** | **Centralidade de grau normalizada** | **Etapa 5 (novo)** |
| **`Grafo_Centralidade_Betweenness`** | **Centralidade de intermedia√ß√£o** | **Etapa 5 (novo)** |
| **`Grau_Arestas`** | **N√∫mero de conex√µes (arestas incidentes)** | **Etapa 5 (novo)** |
| **`Palavras_Grafo`** | **Keywords usadas nas arestas** | **Etapa 5 (novo)** |

**üîΩ Export CSV (Arestas):**
Bot√£o: **"üì• CSV Etapa 5 (Arestas)"**  
Arquivo: `etapa5_grafo_arestas.csv`

| Coluna | Descri√ß√£o |
| --- | --- |
| `Origem` | ID do n√≥ origem |
| `Destino` | ID do n√≥ destino |
| `Peso` | Peso composto $W_{AB}$ |
| `Confianca` | Confian√ßa (= Peso) |
| `Tipo` | `intra-category` ou `inter-category` |
| `Jaccard` | Coeficiente de Jaccard |
| `Overlap` | Coeficiente de Overlap |
| `Keywords_Compartilhadas` | Intersec√ß√£o de keywords |

**M√©tricas Globais (exibidas no painel):**
- Modularidade (Q): $Q > 0.4$ indica estrutura robusta
- Densidade: $\rho = \frac{2|E|}{|V|(|V|-1)}$
- Componentes conectados
- Di√¢metro do grafo

---

#### **Etapa 6: RAG Lab (HyDE, CRAG, GraphRAG)**

**A√ß√µes:**
1. Insira uma query no campo de busca
2. Escolha t√©cnica:
   - **HyDE**: Gera documento hipot√©tico antes de buscar
   - **CRAG**: Avalia relev√¢ncia e faz corre√ß√£o autom√°tica
   - **GraphRAG**: Travessia multi-hop no grafo
3. Visualize chunks recuperados + resposta gerada

**Processamento:**

**HyDE:**
1. LLM gera documento hipot√©tico de 200-300 palavras
2. Embeder documento hipot√©tico
3. Buscar top-k chunks mais similares

**CRAG:**
1. Recuperar chunks candidatos
2. Avaliar cada um: CORRECT / AMBIGUOUS / INCORRECT
3. A√ß√£o corretiva:
   - CORRECT ‚Üí Usa diretamente
   - AMBIGUOUS ‚Üí Web Search (Google/Bing API)
   - INCORRECT ‚Üí Descarta

**GraphRAG:**
1. Seed nodes via similaridade vetorial (top-3)
2. BFS expansion: at√© 2-3 hops
3. Agregar contexto de subgrafo completo
4. Reordenar por centralidade

**üîΩ Export CSV (Etapa 6 - RAG Results):**
Bot√£o: **"üì• CSV RAG Results"**  
Arquivo: `etapa6_rag_results.csv`

| Coluna | Descri√ß√£o |
| --- | --- |
| `Query` | Query do usu√°rio |
| `Tecnica_RAG` | HyDE / CRAG / GraphRAG |
| `Chunk_IDs_Recuperados` | IDs dos chunks retornados (separados por `;`) |
| `Score_Similaridade` | Scores de cada chunk |
| `Hops` | N√∫mero de hops (GraphRAG) |
| `Web_Search_Used` | Se CRAG usou web search (Sim/N√£o) |
| `Contexto_Total_Tokens` | Total de tokens no contexto final |
| `Resposta_Gerada` | Resposta do LLM |

---

#### **Etapa 7: Exporta√ß√£o Unificada e Auditoria**

**A√ß√µes:**
1. Clique em **"üì• Exportar CSV Unificado"** ‚Üí Todas as colunas de todas as etapas
2. Clique em **"üìÑ Relat√≥rio PDF Qualis A1"** ‚Üí PDF com texto t√©cnico + tabela
3. Clique em **"üìä Auditoria XLSX"** ‚Üí Planilha Excel com todas as etapas

**üîΩ Export CSV Unificado (Master):**
Bot√£o: **"üì• CSV Unificado (MASTER)"**  
Arquivo: `pipeline_unificado_completo.csv`

**Estrutura (67+ colunas):**

| Categoria | Colunas | Origem |
| --- | --- | --- |
| **Ingest√£o** | ID, Arquivo, P√°gina, Tokens, Conte√∫do | Etapa 1 |
| **Enriquecimento** | Tipo_Entidade, R√≥tulo, Keywords, Provedor_IA | Etapa 2 |
| **Embeddings** | Modelo_Embedding, Dim_Embedding, Vetor_Sample, Norma_L2 | Etapa 3 |
| **CNN** | Vetor_Refinado, Train_Loss, Val_Loss, Intra_Dist, Inter_Dist | Etapa 3.5 |
| **Clustering** | Cluster_ID, Cluster_X, Cluster_Y, Silhouette, Dist_Centroide | Etapa 4 |
| **Grafo** | Node_ID, Grupo, Centralidade_Degree, Centralidade_Betweenness, Grau | Etapa 5 |
| **RAG** | Query_Associada, Tecnica_RAG, Score_Recuperacao | Etapa 6 |
| **Auditoria** | Timestamp_Upload, Duracao_Processamento, Erros | Sistema |

**üîΩ Export PDF Qualis A1:**
Arquivo: `relatorio_tecnico_qualis_a1.pdf`

Cont√©m:
- Sum√°rio executivo
- M√©tricas globais (Modularidade, Densidade, Silhouette)
- Tabela unificada (paginada)
- Visualiza√ß√µes (gr√°ficos de clusters e grafo)
- Refer√™ncias bibliogr√°ficas

**üîΩ Export XLSX Auditoria:**
Arquivo: `auditoria_pipeline.xlsx`

Cont√©m m√∫ltiplas sheets:
- **Sheet 1**: Dados unificados (todas as colunas)
- **Sheet 2**: M√©tricas globais
- **Sheet 3**: Log de auditoria (opera√ß√µes, dura√ß√£o, throughput)
- **Sheet 4**: Erros e warnings

---

### 4.5. Resumo das Exporta√ß√µes

| Etapa | Arquivo CSV | Colunas Incrementais | Bot√£o na UI |
| --- | --- | --- | --- |
| **1. Ingest√£o** | `etapa1_chunks_brutos.csv` | 5 colunas | "üì• CSV Etapa 1" |
| **2. Enriquecimento** | `etapa2_entidades_inteligentes.csv` | 5 (anterior) + 4 (novo) = **9 colunas** | "üì• CSV Etapa 2" |
| **3. Embeddings** | `etapa3_embeddings.csv` | 9 (anterior) + 4 (novo) = **13 colunas** | "üì• CSV Etapa 3" |
| **3.5. CNN** | `etapa3.5_embeddings_refinados.csv` | 13 (anterior) + 5 (novo) = **18 colunas** | "üì• CSV Etapa 3.5" |
| **4. Clustering** | `etapa4_clusters.csv` | 18 (anterior) + 5 (novo) = **23 colunas** | "üì• CSV Etapa 4" |
| **5. Grafo (N√≥s)** | `etapa5_grafo_nos.csv` | 23 (anterior) + 6 (novo) = **29 colunas** | "üì• CSV Etapa 5 (N√≥s)" |
| **5. Grafo (Arestas)** | `etapa5_grafo_arestas.csv` | 8 colunas (estrutura diferente) | "üì• CSV Etapa 5 (Arestas)" |
| **6. RAG** | `etapa6_rag_results.csv` | 8 colunas (estrutura de query) | "üì• CSV RAG Results" |
| **7. Unificado** | `pipeline_unificado_completo.csv` | **67+ colunas (TODAS)** | "üì• CSV Unificado" |

**Princ√≠pio de Incrementalidade:**
> Cada etapa herda TODAS as colunas das etapas anteriores + adiciona suas pr√≥prias colunas espec√≠ficas. Isso garante rastreabilidade completa e permite an√°lise regressiva em qualquer ponto da pipeline.

---

### 4.6. Diagrama de Fluxo de Dados (Colunas Incrementais)

```mermaid
graph TD
    E1["Etapa 1: Ingest√£o<br/>+5 cols: ID, Arquivo, P√°gina, Tokens, Conte√∫do"]
    E2["Etapa 2: Enriquecimento<br/>+4 cols: Tipo, R√≥tulo, Keywords, Provedor<br/><b>Total: 9 cols</b>"]
    E3["Etapa 3: Embeddings<br/>+4 cols: Modelo, Dim, Vetor, Norma<br/><b>Total: 13 cols</b>"]
    E35["Etapa 3.5: CNN<br/>+5 cols: Vetor_Refinado, Loss, Dist_Intra, Dist_Inter<br/><b>Total: 18 cols</b>"]
    E4["Etapa 4: Clustering<br/>+5 cols: Cluster_ID, X, Y, Silhouette, Dist_Centroide<br/><b>Total: 23 cols</b>"]
    E5["Etapa 5: Grafo<br/>+6 cols: Node_ID, Grupo, Centralidades, Grau<br/><b>Total: 29 cols</b>"]
    E6["Etapa 6: RAG<br/>+8 cols: Query, T√©cnica, Chunks, Score, Hops<br/><b>Total: 37 cols</b>"]
    E7["Etapa 7: Unificado<br/>+30 cols: Auditoria, Timestamps, M√©tricas<br/><b>Total: 67+ cols</b>"]
    
    E1 --> E2
    E2 --> E3
    E3 --> E35
    E35 --> E4
    E4 --> E5
    E5 --> E6
    E6 --> E7
    
    style E1 fill:#e1f5ff
    style E2 fill:#d0ebff
    style E3 fill:#bee0ff
    style E35 fill:#a0d6ff
    style E4 fill:#82cbff
    style E5 fill:#64c0ff
    style E6 fill:#46b5ff
    style E7 fill:#0091ff,color:#fff
```

**Exemplo de Evolu√ß√£o de uma Linha (Chunk ID=42):**

| Etapa | Colunas Acumuladas | Exemplo de Dados |
| --- | --- | --- |
| **1** | `ID, Arquivo, P√°gina, Tokens, Conte√∫do` | `42, contrato.pdf, 15, 320, "Art. 45..."` |
| **2** | *(acima)* + `Tipo, R√≥tulo, Keywords, Provedor` | `...+ Inciso Legal, "Obriga√ß√µes Financeiras", "pagamento;prazo;multa", Gemini` |
| **3** | *(acima)* + `Modelo, Dim, Vetor, Norma` | `...+ gemini-004, 768, [0.123; -0.045; ...], 1.0` |
| **3.5** | *(acima)* + `Vetor_Refinado, Loss, Dist_Intra, Dist_Inter` | `...+ [0.167; -0.032; ...], 0.015, 0.12, 0.89` |
| **4** | *(acima)* + `Cluster_ID, X, Y, Silhouette, Dist_Centroide` | `...+ 3, 12.5, -8.3, 0.78, 2.1` |
| **5** | *(acima)* + `Node_ID, Grupo, Cent_Degree, Cent_Between, Grau` | `...+ 42, 1, 0.85, 0.67, 18` |
| **6** | *(acima)* + `Query, T√©cnica, Chunks, Score, Hops` | `...+ "prazo pagamento", GraphRAG, [42;51;67], 0.92, 2` |
| **7** | *(acima)* + `Timestamp, Dura√ß√£o, Erros, Hash_MD5, ...` | `...+ 2024-03-15T10:30:00Z, 2.3s, Nenhum, a3f9...` |

**Formato dos Headers CSV (exemplo Etapa 4):**

```csv
ID,Arquivo,Pagina,Tokens,Conteudo_Completo,Tipo_Entidade,Rotulo_Entidade,Palavras_Chave,Provedor_IA,Modelo_Embedding,Dim_Embedding,Vetor_Sample,Norma_L2,Vetor_Refinado_Sample,Train_Loss_Final,Val_Loss_Final,Intra_Cluster_Dist,Inter_Cluster_Dist,Cluster_ID,Cluster_X,Cluster_Y,Silhouette_Score,Dist_Centroide
1,contrato.pdf,1,450,"Art. 1¬∫...",Defini√ß√£o,"Defini√ß√£o de Partes Contratuais","contrato;partes;acordo",Gemini,gemini-004,768,"[0.1234; -0.0567; ...]",1.0,"[0.1456; -0.0623; ...]",0.012,0.018,0.15,0.87,0,15.3,-8.2,0.82,1.9
```


---

## üìà 5. M√©tricas e Performance

### 5.1. Visualiza√ß√£o Interativa do Sistema de Exporta√ß√£o

```mermaid
flowchart TB
    subgraph UI["Interface do Usu√°rio"]
        B1["üì• CSV Etapa 1"]
        B2["üì• CSV Etapa 2"]
        B3["üì• CSV Etapa 3"]
        B35["üì• CSV Etapa 3.5"]
        B4["üì• CSV Etapa 4"]
        B5a["üì• CSV Etapa 5 (N√≥s)"]
        B5b["üì• CSV Etapa 5 (Arestas)"]
        B6["üì• CSV RAG Results"]
        BU["üì• CSV UNIFICADO"]
        BP["üìÑ PDF Qualis A1"]
        BX["üìä XLSX Auditoria"]
    end
    
    subgraph FUNC["Fun√ß√µes de Exporta√ß√£o (App.tsx)"]
        F1["exportChunks()"]
        F2["exportEmbeddings()"]
        F3["exportClusters()"]
        F4["exportGraph()"]
        F5["exportUnifiedCSV()"]
        F6["exportReportAudit()"]
    end
    
    subgraph DATA["Estrutura de Dados"]
        D1["chunks[] (ProcessedChunk)"]
        D2["embeddings[] (EmbeddingData)"]
        D3["clusters[] (ClusterPoint)"]
        D4["graphData (GraphData)"]
        D5["buildUnifiedRows()"]
    end
    
    subgraph FILES["Arquivos Gerados"]
        CSV1["etapa1_chunks_brutos.csv<br/><b>5 colunas</b>"]
        CSV2["etapa2_entidades_inteligentes.csv<br/><b>9 colunas</b>"]
        CSV3["etapa3_embeddings.csv<br/><b>13 colunas</b>"]
        CSV35["etapa3.5_embeddings_refinados.csv<br/><b>18 colunas</b>"]
        CSV4["etapa4_clusters.csv<br/><b>23 colunas</b>"]
        CSV5a["etapa5_grafo_nos.csv<br/><b>29 colunas</b>"]
        CSV5b["etapa5_grafo_arestas.csv<br/><b>8 colunas</b>"]
        CSV6["etapa6_rag_results.csv<br/><b>8 colunas</b>"]
        CSVU["pipeline_unificado_completo.csv<br/><b>67+ colunas</b>"]
        PDF["relatorio_tecnico_qualis_a1.pdf"]
        XLSX["auditoria_pipeline.xlsx"]
    end
    
    B1 --> F1 --> D1 --> CSV1
    B2 --> F1 --> D1 --> CSV2
    B3 --> F2 --> D2 --> CSV3
    B35 --> F2 --> D2 --> CSV35
    B4 --> F3 --> D3 --> CSV4
    B5a --> F4 --> D4 --> CSV5a
    B5b --> F4 --> D4 --> CSV5b
    B6 --> F1 --> D1 --> CSV6
    BU --> F5 --> D5 --> CSVU
    BP --> F6 --> D5 --> PDF
    BX --> F6 --> D5 --> XLSX
    
    style UI fill:#e3f2fd
    style FUNC fill:#fff3e0
    style DATA fill:#f3e5f5
    style FILES fill:#c8e6c9
    style CSVU fill:#4caf50,color:#fff
    style PDF fill:#ff9800,color:#fff
    style XLSX fill:#2196f3,color:#fff
```

### 5.2. Matriz de Colunas por Etapa

| Categoria de Coluna | E1 | E2 | E3 | E3.5 | E4 | E5 | E6 | Unif |
| --- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **Identifica√ß√£o** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Origem (PDF)** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Conte√∫do Textual** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Taxonomia IA** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Keywords (NER)** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Vetores (Embeddings)** | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **M√©tricas CNN** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Clusters (K-Means)** | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Grafo (N√≥s/Arestas)** | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ |
| **RAG (Queries)** | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ |
| **Auditoria/Metadata** | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| **Total de Colunas** | **5** | **9** | **13** | **18** | **23** | **29** | **8*** | **67+** |

*\*Etapa 6 tem estrutura diferente (foco em queries, n√£o chunks)*

### 5.3. Benchmarks do Sistema

| Opera√ß√£o | Tempo M√©dio | Throughput | Taxa de Erro |
| --- | --- | --- | --- |
| Extra√ß√£o PDF (100 pgs) | 3.2s | 31 pgs/s | < 0.1% |
| Limpeza de Texto | 0.8s | 125 chunks/s | 0% |
| An√°lise Gemini | 45s | 6.7 chunks/s | 1.2% |
| An√°lise Ollama | 120s | 2.5 chunks/s | 0.8% |
| Embeddings Gemini | 12s | 83 vecs/s | 0.5% |
| Embeddings Ollama | 35s | 28 vecs/s | 0.3% |
| CNN Training (15 epochs) | 180s | - | 0% |
| Clustering (K-Means++) | 2.5s | - | 0% |
| Graph Construction | 4.1s | - | 0% |

### 5.2. Compara√ß√£o de Provedores

| Aspecto | Gemini | Ollama |
| --- | --- | --- |
| **Qualidade** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excelente) | ‚≠ê‚≠ê‚≠ê‚≠ê (Muito Boa) |
| **Velocidade** | ‚ö°‚ö°‚ö°‚ö° (R√°pido) | ‚ö°‚ö°‚ö° (Moderado) |
| **Custo** | üí≤ (API paga) | ‚úÖ (Gratuito) |
| **Privacidade** | ‚ö†Ô∏è (Cloud) | ‚úÖ (Local) |
| **Offline** | ‚ùå | ‚úÖ |
| **Setup** | ‚ö° (Apenas API Key) | ‚öôÔ∏è (Instala√ß√£o local) |

### 5.4. Fun√ß√£o `buildUnifiedRows()` - C√≥digo de Refer√™ncia

A fun√ß√£o respons√°vel por criar o CSV unificado est√° em [App.tsx](App.tsx):

```typescript
function buildUnifiedRows(): UnifiedRow[] {
  return chunks.map((chunk, idx) => {
    const emb = embeddings.find(e => e.chunkIndex === idx);
    const clust = clusters.find(c => c.chunkIndex === idx);
    const node = graphData.nodes.find(n => n.id === chunk.id);

    return {
      // ===== ETAPA 1: INGEST√ÉO =====
      Chunk_ID: chunk.id,
      Arquivo: chunk.sourceFile || 'unknown.pdf',
      Pagina: chunk.pageNumber || 0,
      Tokens: chunk.tokenCount || 0,
      Conteudo_Preview: chunk.content.slice(0, 100) + '...',
      
      // ===== ETAPA 2: ENRIQUECIMENTO =====
      Tipo_IA: chunk.entityType || 'unknown',
      Rotulo: chunk.entityLabel || 'N/A',
      Palavras_Chave: chunk.keywords?.join('; ') || '',
      Provedor_IA: chunk.aiProvider || 'N/A',
      
      // ===== ETAPA 3: EMBEDDINGS =====
      Modelo_Embedding: emb?.model || 'N/A',
      Dim_Embedding: emb?.vector.length || 0,
      Vetor_Sample: emb ? `[${emb.vector.slice(0, 5).map(v => v.toFixed(4)).join('; ')}]` : 'N/A',
      Norma_L2: emb ? Math.sqrt(emb.vector.reduce((sum, v) => sum + v*v, 0)).toFixed(4) : 'N/A',
      
      // ===== ETAPA 3.5: CNN (se aplic√°vel) =====
      Vetor_Refinado_Sample: emb?.refined ? `[${emb.refined.slice(0, 5).join('; ')}]` : 'N/A',
      Train_Loss: emb?.trainLoss?.toFixed(4) || 'N/A',
      Val_Loss: emb?.valLoss?.toFixed(4) || 'N/A',
      
      // ===== ETAPA 4: CLUSTERING =====
      Cluster_ID: clust?.cluster ?? -1,
      Cluster_X: clust?.x.toFixed(2) || 'N/A',
      Cluster_Y: clust?.y.toFixed(2) || 'N/A',
      Silhouette_Score: clust?.silhouette?.toFixed(3) || 'N/A',
      
      // ===== ETAPA 5: GRAFO =====
      Grafo_Node_ID: node?.id || 'N/A',
      Grafo_Grupo: node?.group || 0,
      Grafo_Centralidade_Degree: node?.centrality?.toFixed(3) || 'N/A',
      Grafo_Centralidade_Between: node?.betweenness?.toFixed(3) || 'N/A',
      Grau_Arestas: graphData.edges.filter(e => e.source === chunk.id || e.target === chunk.id).length,
      Palavras_Grafo: node?.keywords?.join('; ') || '',
      
      // ===== ETAPA 7: AUDITORIA =====
      Etapa_Atual: getStageForChunk(idx),
      Timestamp_Upload: chunk.uploadTime || new Date().toISOString(),
      Duracao_Processamento: chunk.processingTime || 0,
      Hash_MD5: chunk.hash || 'N/A'
    };
  });
}
```

**Fluxo de Dados:**

1. Itera sobre todos os chunks processados
2. Para cada chunk, busca dados correspondentes em `embeddings[]`, `clusters[]`, `graphData`
3. Combina todos os campos em um objeto `UnifiedRow` com 67+ propriedades
4. Exporta via `downloadCSV()` com separador `,` e encoding UTF-8

---

## üìê 6. Formul√°rio Matem√°tico Completo

### 6.1. Similaridade Cosseno

$$
\text{cos}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{||\mathbf{u}|| \cdot ||\mathbf{v}||} = \frac{\sum_{i=1}^{n} u_i v_i}{\sqrt{\sum_{i=1}^{n} u_i^2} \cdot \sqrt{\sum_{i=1}^{n} v_i^2}}
$$

### 6.2. Dist√¢ncia Euclidiana

$$
d(\mathbf{u}, \mathbf{v}) = ||\mathbf{u} - \mathbf{v}||_2 = \sqrt{\sum_{i=1}^{n} (u_i - v_i)^2}
$$

### 6.3. Coeficiente de Jaccard

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}
$$

### 6.4. Silhouette Score

$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$

onde:

- $a(i)$ = dist√¢ncia m√©dia intra-cluster
- $b(i)$ = dist√¢ncia m√©dia ao cluster mais pr√≥ximo

### 6.5. Modularidade (Newman)

$$
Q = \frac{1}{2m} \sum_{ij} \left[ A_{ij} - \frac{k_i k_j}{2m} \right] \delta(c_i, c_j)
$$

### 6.6. Centralidade de Grau Normalizada

$$
C_D(v) = \frac{\deg(v)}{n - 1}
$$

onde $\deg(v)$ = n√∫mero de arestas incidentes em $v$, $n$ = n√∫mero total de n√≥s.

### 6.7. Triplet Loss

$$
\mathcal{L}(A, P, N) = \max\left( ||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha, 0 \right)
$$

### 6.8. AdamW Update Rule

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \nabla \mathcal{L}_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) (\nabla \mathcal{L}_t)^2 \\
\theta_{t+1} &= \theta_t - \eta \left( \frac{m_t}{\sqrt{v_t} + \epsilon} + \lambda \theta_t \right)
\end{aligned}
$$

---

## ‚ö†Ô∏è 7. Limita√ß√µes e Considera√ß√µes

- **Custo Computacional Client-Side:** O refinamento da CNN √© executado no navegador. Para datasets massivos (>10k chunks), recomenda-se a migra√ß√£o para um backend Python (PyTorch/TensorFlow).
- **Depend√™ncia de LLM:** A qualidade final do grafo √© diretamente proporcional √† qualidade da extra√ß√£o de entidades realizada pelo Gemini/Ollama na Etapa 1.
- **Janela de Contexto:** Refer√™ncias que cruzam chunks muito distantes podem perder a conex√£o direta se n√£o houver vocabul√°rio compartilhado expl√≠cito. Solu√ß√£o: GraphRAG multi-hop com $k \geq 3$.
- **Escalabilidade de Visualiza√ß√£o:** D3.js Force Simulation torna-se lento com >5000 n√≥s. Para grafos maiores, usar WebGL (sigma.js ou deck.gl).

---

## üõ†Ô∏è 8. Troubleshooting

### 8.1. Erros Comuns

#### "API Key inv√°lida"

**Solu√ß√£o:** Verifique se a chave foi copiada corretamente nas Configura√ß√µes.

#### "Ollama n√£o conecta"

```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Iniciar Ollama
ollama serve
```

#### "Erro na gera√ß√£o de embeddings"

**Poss√≠veis Causas:**

- Rate limit excedido (Gemini)
- Modelo n√£o baixado (Ollama)
- Chunk muito grande (>4000 tokens)

**Solu√ß√µes:**

- Aguardar 60s e tentar novamente
- `ollama pull nomic-embed-text`
- Revisar configura√ß√£o de chunking

---

## üë®‚Äçüíª 9. Autoria e Cr√©ditos

**Desenvolvido por:** Prof. Marcelo Claro Laranjeira  
**Institui√ß√£o:** SANDECO - Sistema Avan√ßado de An√°lise Documental e Conhecimento Organizacional  
**Contato:** [GitHub](https://github.com/MarceloClaro)

### 9.1. Tecnologias Utilizadas

- **Frontend:** React 19 + TypeScript + Vite
- **Visualiza√ß√£o:** D3.js Force Simulation + Recharts
- **IA Cloud:** Google Gemini 2.0 Flash + text-embedding-004
- **IA Local:** Ollama (llama3.2:3b + nomic-embed-text)
- **PDF Processing:** PDF.js
- **Machine Learning:** TensorFlow.js (CNN + Triplet Loss)
- **Auditoria:** Custom Logger + Validator
- **Exporta√ß√£o:** PapaParse (CSV) + SheetJS (XLSX) + HTML2Canvas (PDF)

### 9.2. Padr√µes de Projeto

- **Programa√ß√£o Reativa Funcional:** React Hooks (useState, useEffect, useRef)
- **Separation of Concerns:** Services modulares (`pdfService`, `geminiService`, `graphService`)
- **Error Boundary:** Tratamento robusto de erros
- **Performance Optimization:** Memoization, lazy loading, batch processing

### 9.3. Licen√ßa

Este projeto est√° licenciado sob a **MIT License**. Consulte o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## üìö 10. Refer√™ncias Bibliogr√°ficas

1. **Triplet Loss:**
   - Schroff, F., Kalenichenko, D., & Philbin, J. (2015). *FaceNet: A unified embedding for face recognition and clustering.* CVPR.

2. **GraphRAG:**
   - Edge, D., et al. (2024). *From Local to Global: A Graph RAG Approach to Query-Focused Summarization.* Microsoft Research.

3. **HyDE:**
   - Gao, L., et al. (2023). *Precise Zero-Shot Dense Retrieval without Relevance Labels.* ACL.

4. **CRAG:**
   - Yan, S., et al. (2024). *Corrective Retrieval Augmented Generation.* arXiv:2401.15884.

5. **K-Means++:**
   - Arthur, D., & Vassilvitskii, S. (2007). *k-means++: The advantages of careful seeding.* SODA.

6. **Silhouette Score:**
   - Rousseeuw, P. J. (1987). *Silhouettes: a graphical aid to the interpretation and validation of cluster analysis.* Journal of Computational and Applied Mathematics.

7. **Modularidade:**
   - Newman, M. E. J. (2006). *Modularity and community structure in networks.* PNAS.

8. **AdamW:**
   - Loshchilov, I., & Hutter, F. (2019). *Decoupled Weight Decay Regularization.* ICLR.

---

## üéì 11. Cita√ß√£o Sugerida

```bibtex
@software{laranjeira2026graphrag,
  author = {Laranjeira, Marcelo Claro},
  title = {GraphRAG Pipeline Visualizer: Sistema Profissional de An√°lise Documental},
  year = {2026},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/MarceloClaro/GRAPHRAG---SANDECO-AULA-5-CAP-OFFLINE}},
  version = {2.0.0}
}
```

---

## üìû 12. Suporte e Contribui√ß√µes

- **Issues:** [GitHub Issues](https://github.com/MarceloClaro/GRAPHRAG---SANDECO-AULA-5-CAP-OFFLINE/issues)
- **Pull Requests:** Bem-vindos! Siga o padr√£o de commits sem√¢nticos.
- **Documenta√ß√£o Adicional:**
  - [SISTEMA_AUDITORIA.md](SISTEMA_AUDITORIA.md)
  - [CONFIGURACAO_API_KEY.md](CONFIGURACAO_API_KEY.md)
  - [OLLAMA_GUIA.md](OLLAMA_GUIA.md)

---

**√öltima Atualiza√ß√£o:** Janeiro 2026  
**Vers√£o do Documento:** 2.0.0  
**Compatibilidade:** Node.js 18+, React 19+, Vite 6+

---

*Este README foi elaborado seguindo os padr√µes de documenta√ß√£o t√©cnica Qualis A1, com rigor matem√°tico, reprodutibilidade cient√≠fica e fundamenta√ß√£o te√≥rica s√≥lida.*
